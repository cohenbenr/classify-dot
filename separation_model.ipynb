{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -e 'git://github.com/nandanrao/embed-software.git#egg=embed_software'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --quiet fuzzywuzzy gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gcsfs import GCSFileSystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "#import torchtext\n",
    "\n",
    "from validation.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import StarSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 10000\n",
    "SOC_LEVEL = 3\n",
    "OUTPUT_WEIGHTS = 'data/separation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(fmt=\"%(asctime)s %(levelname)s: %(message)s\", \n",
    "                          datefmt=\"%Y-%m-%d - %H:%M:%S\")\n",
    "fh = logging.FileHandler(\"separation_model.log\", \"w\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_test(l_batch, r_batch, neg_batch):\n",
    "    global nan_break\n",
    "    \n",
    "    l_test = np.isnan(np.mean(l_batch.detach().cpu().numpy()))\n",
    "    r_test = np.isnan(np.mean(r_batch.detach().cpu().numpy()))\n",
    "    neg_test = np.isnan(np.mean(neg_batch.detach().cpu().numpy()))\n",
    "    if l_test or r_test or neg_test:\n",
    "        nan_break = True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('Pulling Indeed data for sample size %s' % SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job ads data\n",
    "indeed = get_indeed_texts('../data/us/everything.csv',use_gcs=True,nrows=SAMPLE_SIZE)\n",
    "indeed = indeed['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = indeed.copy()\n",
    "del indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DOT classifications data\n",
    "dot, dot_labs = dot_train_data(SOC_LEVEL)\n",
    "\n",
    "dot = dot.reset_index(drop=True)\n",
    "dot_labs = dot_labs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('About to train vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer(min_df = 5,\n",
    "                             max_df = .99)\n",
    "Vectorizer.fit(train)\n",
    "\n",
    "train_vocab = Vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12431"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('Trained Vocab of size %s' % str(len(train_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the file\n",
    "with open(OUTPUT_WEIGHTS + 'train_vocab_%s' % SAMPLE_SIZE, 'wb') as f:\n",
    "    pickle.dump(train_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start from file:\n",
    "# with open('data/separation/weights_100000', 'rb') as f:\n",
    "#     embeddings = pickle.load(f)\n",
    "\n",
    "# print(embeddings.shape)\n",
    "# embeddings = torch.FloatTensor(embeddings)\n",
    "# embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "\n",
    "# with open('data/separation/train_vocab_100000', 'rb') as f:\n",
    "#     vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StarSpace(\n",
       "  (embeddings): Embedding(12431, 100, max_norm=20)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StarSpace(\n",
    "    d_embed=100,\n",
    "    vocabulary=train_vocab,\n",
    "    k_neg = 10)\n",
    "#     input_embedding = embeddings)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR params\n",
    "clusters_to_sample = 10\n",
    "dot_y_enc = torch.tensor(np.unique(dot_labs,return_inverse=True)[1]).to(device) #encoded\n",
    "num_clusters = len(set(dot_labs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(100,num_clusters)\n",
    "LR.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#lr_optim = torch.optim.SGD(LR.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .01\n",
    "opt = torch.optim.Adam(list(model.parameters()) + list(LR.parameters()), lr=lr)\n",
    "#opt = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = model.get_positions(train)\n",
    "dot_positions = model.get_positions(dot)\n",
    "\n",
    "for i in range(len(train_pos)):\n",
    "    for j in range(len(train_pos[i])):\n",
    "        train_pos[i][j] = train_pos[i][j].to(device)\n",
    "\n",
    "for i in range(len(dot_positions)):\n",
    "    for j in range(len(dot_positions[i])):\n",
    "        dot_positions[i][j] = dot_positions[i][j].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run parameters\n",
    "epochs = 3\n",
    "print_every = 10\n",
    "log_every = 10\n",
    "batch_size = 100\n",
    "\n",
    "#Losses\n",
    "losses = []\n",
    "separation_losses = []\n",
    "epoch_losses = [1e12]\n",
    "epoch_losses_sep = [100]\n",
    "log.info('Beginning run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "separation avg loss: 4.8241343\n",
      "star avg loss: 6677061.0\n",
      "separation avg loss: 5.038816\n",
      "star avg loss: 5643091.0\n",
      "separation avg loss: 5.3123164\n",
      "star avg loss: 5452008.5\n",
      "separation avg loss: 4.6863766\n",
      "star avg loss: 6088436.0\n",
      "separation avg loss: 5.185051\n",
      "star avg loss: 5824729.5\n",
      "separation avg loss: 4.497126\n",
      "star avg loss: 5648737.0\n",
      "separation avg loss: 4.8049574\n",
      "star avg loss: 5817563.0\n",
      "separation avg loss: 4.448853\n",
      "star avg loss: 5521075.0\n",
      "separation avg loss: 4.541656\n",
      "star avg loss: 5229973.5\n",
      "separation avg loss: 4.493988\n",
      "star avg loss: 5796291.0\n",
      "Finished epoch 0 at Mon Jun 22 20:25:15 2020.\n",
      "best epoch so far!\n",
      "[1000000000000.0, 12755768.0, 7994368.5, 6915227.0, 5681348.5]\n",
      "separation avg loss: 5.0684304\n",
      "star avg loss: 5559606.0\n",
      "separation avg loss: 5.097411\n",
      "star avg loss: 5283095.0\n",
      "separation avg loss: 4.8026743\n",
      "star avg loss: 5342672.5\n",
      "separation avg loss: 4.638465\n",
      "star avg loss: 5612934.0\n",
      "separation avg loss: 4.570048\n",
      "star avg loss: 5093255.0\n",
      "separation avg loss: 4.896729\n",
      "star avg loss: 5384201.0\n",
      "separation avg loss: 5.028816\n",
      "star avg loss: 4864028.0\n",
      "separation avg loss: 4.5725875\n",
      "star avg loss: 4711147.0\n"
     ]
    }
   ],
   "source": [
    "#Real loop\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(len(train_pos)).numpy()\n",
    "    nan_break = False\n",
    "    \n",
    "    for i in range(0,len(train), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch = train_pos[indices]\n",
    "\n",
    "        ###### Forward Pass- StarSpace #############################################################\n",
    "        model.train(); opt.zero_grad()\n",
    "        \n",
    "        l_batch, r_batch, neg_batch = model(batch)\n",
    "        \n",
    "        #Test for nans\n",
    "#         if nan_test(l_batch, r_batch, neg_batch):\n",
    "#             break\n",
    "        \n",
    "        positive_similarity = torch.bmm(l_batch,r_batch.transpose(2,1))\n",
    "        negative_similarity = torch.bmm(l_batch, neg_batch.transpose(2,1)).squeeze(1)\n",
    "\n",
    "        star_loss = torch.sum(torch.clamp(.1 - positive_similarity + negative_similarity, min=0))\n",
    "        \n",
    "        ###### Switch modes- Sep On #################################################################\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in LR.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        ###### Forward pass- Separation #############################################################\n",
    "        clusts = torch.tensor(np.random.choice(num_clusters, clusters_to_sample, False)).to(device)\n",
    "        mask = dot_y_enc.view(1, -1).eq(clusts.view(-1,1)).sum(0)\n",
    "\n",
    "        dot_sample = dot_positions[torch.nonzero(mask).detach().cpu()]\n",
    "        dot_y_sample = dot_y_enc[np.where(mask.cpu())]\n",
    "\n",
    "#         LR = LogisticRegression(100,clusters_to_sample)\n",
    "#         LR.to(device)\n",
    "        new_dots = [torch.cat(doc.tolist()[0]) for doc in dot_sample]\n",
    "        dot_emb = [model.embed_doc(doc) for doc in new_dots]\n",
    "        dot_emb = torch.stack(dot_emb)\n",
    "        dot_emb.to(device)\n",
    "        \n",
    "        lr_steps = 1\n",
    "        for r in range(lr_steps):\n",
    "            opt.zero_grad()\n",
    "\n",
    "            output = LR(dot_emb)\n",
    "            separation_loss = criterion(output,dot_y_sample)\n",
    "            separation_loss.backward(retain_graph=True)\n",
    "            opt.step()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "                \n",
    "        ###### Switch modes- Star On ################################################################\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for param in LR.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ###### Combine Losses/Backward Pass ##########################################################\n",
    "        loss = star_loss + separation_loss\n",
    "\n",
    "        loss.backward();opt.step()\n",
    "\n",
    "        ###### Print/Log #############################################################################\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        separation_losses.append(separation_loss.detach().cpu().numpy())\n",
    "\n",
    "        if i % (print_every*batch_size) == 0:\n",
    "            print('separation avg loss: %s' % str(np.mean(separation_losses[-10:])))\n",
    "            print('star avg loss: %s' % str(np.mean(losses[-10:])))\n",
    "        if i % (log_every*batch_size) == 0:\n",
    "            log.info('separation avg loss: %s' % str(np.mean(separation_losses[-10:])))\n",
    "            log.info('star avg loss: %s' % str(np.mean(losses[-10:])))\n",
    "    \n",
    "    # End of inner loop\n",
    "    if nan_break:\n",
    "        print(\"you've got nans\")\n",
    "        log.warning(\"you've got nans\")\n",
    "        break\n",
    "    \n",
    "    ###### Print/Log #############################################################################\n",
    "    print('Finished epoch %s at %s.' % (epoch,time.ctime()))\n",
    "    log.info(\"Finished epoch %s\" % str(epoch))\n",
    "    \n",
    "    epoch_loss = np.mean(losses[-int(SAMPLE_SIZE/batch_size):])\n",
    "    epoch_loss_sep = np.mean(separation_losses[-int(SAMPLE_SIZE/batch_size):])\n",
    "\n",
    "    if (epoch_loss < min(epoch_losses)) | (len(epoch_losses) == 0):\n",
    "        print('best epoch so far!')\n",
    "        log.info('best epoch so far!')\n",
    "        \n",
    "        weights = model.embeddings.weight\n",
    "        with open(OUTPUT_WEIGHTS + 'weights_best_epoch', 'wb') as f:\n",
    "            pickle.dump(weights.data.detach().cpu().numpy(), f)\n",
    "    \n",
    "    epoch_losses.append(epoch_loss)\n",
    "    epoch_losses_sep.append(epoch_loss_sep)\n",
    "    print(epoch_losses)\n",
    "    print(epoch_losses_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.embeddings.weight\n",
    "with open(OUTPUT_WEIGHTS + 'weights_%s' % SAMPLE_SIZE, 'wb') as f:\n",
    "    pickle.dump(weights.data.detach().cpu().numpy(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made it!\n"
     ]
    }
   ],
   "source": [
    "print('You made it!')\n",
    "log.info('You made it!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 6.61307, 5.0558543, 4.849506]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_losses_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the weights to CSV\n",
    "# weights = model.input_embedding.weight\n",
    "# weights = weights.data.detach().numpy()\n",
    "# np.savetxt(\"weights_%s.csv\" % SAMPLE_SIZE, weights, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
