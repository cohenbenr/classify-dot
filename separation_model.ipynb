{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -e 'git://github.com/nandanrao/embed-software.git#egg=embed_software'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --quiet fuzzywuzzy gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gcsfs import GCSFileSystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "#import torchtext\n",
    "\n",
    "from validation.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import StarSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 10000\n",
    "SOC_LEVEL = 3\n",
    "OUTPUT_WEIGHTS = 'data/separation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(fmt=\"%(asctime)s %(levelname)s: %(message)s\", \n",
    "                          datefmt=\"%Y-%m-%d - %H:%M:%S\")\n",
    "fh = logging.FileHandler(\"separation_model.log\", \"w\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_test(l_batch, r_batch, neg_batch):\n",
    "    global nan_break\n",
    "    \n",
    "    l_test = np.isnan(np.mean(l_batch.detach().cpu().numpy()))\n",
    "    r_test = np.isnan(np.mean(r_batch.detach().cpu().numpy()))\n",
    "    neg_test = np.isnan(np.mean(neg_batch.detach().cpu().numpy()))\n",
    "    if l_test or r_test or neg_test:\n",
    "        nan_break = True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('Pulling Indeed data for sample size %s' % SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job ads data\n",
    "indeed = get_indeed_texts('../data/us/everything.csv',use_gcs=True,nrows=SAMPLE_SIZE)\n",
    "indeed = indeed['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = indeed.copy()\n",
    "del indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DOT classifications data\n",
    "dot, dot_labs = dot_train_data(SOC_LEVEL)\n",
    "\n",
    "dot = dot.reset_index(drop=True)\n",
    "dot_labs = dot_labs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('About to train vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer(min_df = 5,\n",
    "                             max_df = .99)\n",
    "Vectorizer.fit(train)\n",
    "\n",
    "train_vocab = Vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12431"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('Trained Vocab of size %s' % str(len(train_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the file\n",
    "with open(OUTPUT_WEIGHTS + 'train_vocab_%s' % SAMPLE_SIZE, 'wb') as f:\n",
    "    pickle.dump(train_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StarSpace(\n",
       "  (embeddings): Embedding(12431, 100, max_norm=20)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StarSpace(\n",
    "    d_embed=100,\n",
    "    vocabulary=train_vocab,\n",
    "    k_neg = 10)\n",
    "#     input_embedding = embeddings)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR params\n",
    "clusters_to_sample = 10\n",
    "dot_y_enc = torch.tensor(np.unique(dot_labs,return_inverse=True)[1]).to(device) #encoded\n",
    "num_clusters = len(set(dot_labs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(100,num_clusters)\n",
    "LR.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = torch.optim.Adam(list(model.parameters()) + list(LR.parameters()), lr=lr)\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': model.parameters(), 'lr': 1e-2},\n",
    "    {'params': LR.parameters(), 'lr': 1e-2}\n",
    "            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = model.get_positions(train)\n",
    "dot_positions = model.get_positions(dot)\n",
    "\n",
    "for i in range(len(train_pos)):\n",
    "    for j in range(len(train_pos[i])):\n",
    "        train_pos[i][j] = train_pos[i][j].to(device)\n",
    "\n",
    "for i in range(len(dot_positions)):\n",
    "    for j in range(len(dot_positions[i])):\n",
    "        dot_positions[i][j] = dot_positions[i][j].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run parameters\n",
    "epochs = 3\n",
    "print_every = 10\n",
    "log_every = 10\n",
    "batch_size = 100\n",
    "\n",
    "#Losses\n",
    "error_lambdas = dict([['starspace',.1],\n",
    "                      ['separation',.9]])\n",
    "losses = []\n",
    "separation_losses = []\n",
    "epoch_losses = [1e12]\n",
    "epoch_losses_sep = [1e12]\n",
    "log.info('Beginning run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:738: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, bool as_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "star avg loss: 103.778915\n",
      "separation avg loss: 4.0870705\n",
      "star avg loss: 98.53531\n",
      "separation avg loss: 4.0109043\n",
      "star avg loss: 93.81991\n",
      "separation avg loss: 3.8870444\n",
      "star avg loss: 93.966286\n",
      "separation avg loss: 3.8611388\n",
      "star avg loss: 88.04742\n",
      "separation avg loss: 3.7865272\n",
      "star avg loss: 89.978165\n",
      "separation avg loss: 3.7889385\n",
      "star avg loss: 85.53112\n",
      "separation avg loss: 3.7750823\n",
      "star avg loss: 86.63531\n",
      "separation avg loss: 3.5677955\n",
      "star avg loss: 83.481895\n",
      "separation avg loss: 3.4620667\n",
      "Finished epoch 0 at Thu Jun 25 17:23:00 2020.\n",
      "best epoch so far!\n",
      "[1000000000000.0, 90.6501]\n",
      "[1000000000000.0, 3.7850442]\n",
      "star avg loss: 72.34969\n",
      "separation avg loss: 3.6584773\n",
      "star avg loss: 73.39623\n",
      "separation avg loss: 3.4627755\n",
      "star avg loss: 68.97413\n",
      "separation avg loss: 3.4005108\n",
      "star avg loss: 69.465225\n",
      "separation avg loss: 3.4632163\n",
      "star avg loss: 66.5389\n",
      "separation avg loss: 3.36913\n",
      "star avg loss: 67.844246\n",
      "separation avg loss: 3.3363461\n",
      "star avg loss: 70.855515\n",
      "separation avg loss: 3.5005927\n",
      "star avg loss: 64.65505\n",
      "separation avg loss: 3.3828464\n",
      "star avg loss: 65.98988\n",
      "separation avg loss: 3.2852955\n",
      "Finished epoch 1 at Thu Jun 25 17:26:01 2020.\n",
      "best epoch so far!\n",
      "[1000000000000.0, 90.6501, 68.48818]\n",
      "[1000000000000.0, 3.7850442, 3.4272268]\n",
      "star avg loss: 60.50772\n",
      "separation avg loss: 3.4295871\n",
      "star avg loss: 60.97937\n",
      "separation avg loss: 3.1756465\n",
      "star avg loss: 57.430267\n",
      "separation avg loss: 3.1554585\n",
      "star avg loss: 60.120922\n",
      "separation avg loss: 3.1264586\n",
      "star avg loss: 53.99471\n",
      "separation avg loss: 3.1391346\n",
      "star avg loss: 56.36507\n",
      "separation avg loss: 3.2454758\n",
      "star avg loss: 54.804066\n",
      "separation avg loss: 3.0155153\n",
      "star avg loss: 54.439034\n",
      "separation avg loss: 3.2973213\n",
      "star avg loss: 61.53789\n",
      "separation avg loss: 3.1877975\n",
      "Finished epoch 2 at Thu Jun 25 17:29:05 2020.\n",
      "best epoch so far!\n",
      "[1000000000000.0, 90.6501, 68.48818, 57.82406]\n",
      "[1000000000000.0, 3.7850442, 3.4272268, 3.1811469]\n"
     ]
    }
   ],
   "source": [
    "#Real loop\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(len(train_pos)).numpy()\n",
    "    nan_break = False\n",
    "    \n",
    "    for i in range(0,len(train), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch = train_pos[indices]\n",
    "\n",
    "        ###### Forward Pass- StarSpace #############################################################\n",
    "        model.train(); opt.zero_grad()\n",
    "        \n",
    "        l_batch, r_batch, neg_batch = model(batch)\n",
    "        \n",
    "        #Test for nans\n",
    "#         if nan_test(l_batch, r_batch, neg_batch):\n",
    "#             break\n",
    "        \n",
    "        positive_similarity = torch.bmm(l_batch,r_batch.transpose(2,1))\n",
    "        negative_similarity = torch.mean(torch.bmm(l_batch, neg_batch.transpose(2,1)).squeeze(1),1)\n",
    "\n",
    "        star_loss = torch.sum(torch.clamp(.1 - positive_similarity + \\\n",
    "                                          negative_similarity, min=0)) * error_lambdas['starspace']\n",
    "        \n",
    "        ###### Forward pass- Separation #############################################################\n",
    "        clusts = torch.tensor(np.random.choice(num_clusters, clusters_to_sample, False)).to(device)\n",
    "        mask = dot_y_enc.view(1, -1).eq(clusts.view(-1,1)).sum(0)\n",
    "\n",
    "        dot_sample = dot_positions[torch.nonzero(mask).detach().cpu()]\n",
    "        dot_y_sample = dot_y_enc[np.where(mask.cpu())]\n",
    "\n",
    "#         LR = LogisticRegression(100,clusters_to_sample)\n",
    "#         LR.to(device)\n",
    "        dot_emb = [model.embed_doc(torch.cat(doc.tolist()[0])) for doc in dot_sample]\n",
    "        dot_emb = torch.stack(dot_emb)\n",
    "        dot_emb.to(device)\n",
    "        \n",
    "        #opt.zero_grad()\n",
    "\n",
    "        output = LR(dot_emb)\n",
    "        separation_loss = criterion(output,dot_y_sample) * error_lambdas['separation']\n",
    "                \n",
    "        ###### Combine Losses/Backward Pass ##########################################################\n",
    "        loss = star_loss + separation_loss\n",
    "\n",
    "        loss.backward();opt.step()\n",
    "\n",
    "        ###### Print/Log #############################################################################\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        separation_losses.append(separation_loss.detach().cpu().numpy())\n",
    "\n",
    "        if (i % (print_every*batch_size) == 0) & (i > 0):\n",
    "            print('star avg loss: %s' % str(np.mean(losses[-10:])))\n",
    "            print('separation avg loss: %s' % str(np.mean(separation_losses[-10:])))\n",
    "        if (i % (log_every*batch_size) == 0) & (i > 0):\n",
    "            log.info('star avg loss: %s' % str(np.mean(losses[-10:])))\n",
    "            log.info('separation avg loss: %s' % str(np.mean(separation_losses[-10:])))\n",
    "    \n",
    "    # End of inner loop\n",
    "    if nan_break:\n",
    "        print(\"you've got nans\")\n",
    "        log.warning(\"you've got nans\")\n",
    "        break\n",
    "    \n",
    "    ###### Print/Log #############################################################################\n",
    "    print('Finished epoch %s at %s.' % (epoch,time.ctime()))\n",
    "    log.info(\"Finished epoch %s\" % str(epoch))\n",
    "    \n",
    "    epoch_loss = np.mean(losses[-int(SAMPLE_SIZE/batch_size):])\n",
    "    epoch_loss_sep = np.mean(separation_losses[-int(SAMPLE_SIZE/batch_size):])\n",
    "\n",
    "    if (epoch_loss < min(epoch_losses)) | (len(epoch_losses) == 0):\n",
    "        print('best epoch so far!')\n",
    "        log.info('best epoch so far!')\n",
    "        log.info('')\n",
    "        \n",
    "        weights = model.embeddings.weight\n",
    "        with open(OUTPUT_WEIGHTS + 'weights_best_epoch', 'wb') as f:\n",
    "            pickle.dump(weights.data.detach().cpu().numpy(), f)\n",
    "    \n",
    "    epoch_losses.append(epoch_loss)\n",
    "    epoch_losses_sep.append(epoch_loss_sep)\n",
    "    print(epoch_losses)\n",
    "    print(epoch_losses_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.embeddings.weight\n",
    "with open(OUTPUT_WEIGHTS + 'weights_%s' % SAMPLE_SIZE, 'wb') as f:\n",
    "    pickle.dump(weights.data.detach().cpu().numpy(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made it!\n"
     ]
    }
   ],
   "source": [
    "print('You made it!')\n",
    "log.info('You made it!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000000000000.0, 90.6501, 68.48818, 57.82406]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000000000000.0, 3.7850442, 3.4272268, 3.1811469]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_losses_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the weights to CSV\n",
    "# weights = model.input_embedding.weight\n",
    "# weights = weights.data.detach().numpy()\n",
    "# np.savetxt(\"weights_%s.csv\" % SAMPLE_SIZE, weights, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
