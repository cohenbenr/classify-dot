{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -e 'git://github.com/nandanrao/embed-software.git#egg=embed_software'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gcsfs import GCSFileSystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "#import torchtext\n",
    "\n",
    "from validation.data import *\n",
    "\n",
    "from src.model import StarSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(fmt=\"%(asctime)s %(levelname)s: %(message)s\", \n",
    "                          datefmt=\"%Y-%m-%d - %H:%M:%S\")\n",
    "fh = logging.FileHandler(\"separation_model.log\", \"w\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 10000\n",
    "SOC_LEVEL = 3\n",
    "OUTPUT_WEIGHTS = 'data/separation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('Pulling Indeed data for sample size %s' % SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job ads data\n",
    "indeed = get_indeed_texts('../data/us/everything.csv',use_gcs=True,nrows=SAMPLE_SIZE)\n",
    "indeed = indeed['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = indeed.copy()\n",
    "del indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32289,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get DOT classifications data\n",
    "dot, dot_labs = dot_train_data(SOC_LEVEL)\n",
    "dot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('About to train vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer(min_df = 5,\n",
    "                             max_df = .95)\n",
    "Vectorizer.fit(train)\n",
    "\n",
    "train_vocab = Vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the file\n",
    "with open(OUTPUT_WEIGHTS + 'train_vocab_%s' % SAMPLE_SIZE, 'wb') as f:\n",
    "    pickle.dump(train_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(OUTPUT_WEIGHTS + \"train_vocab_%s\" % SAMPLE_SIZE,\"rb\") as f:\n",
    "#     train_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12428"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('Trained Vocab of size %s' % str(len(train_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StarSpace(\n",
    "    d_embed=100,\n",
    "    vocabulary=train_vocab,\n",
    "    k_neg = 10,\n",
    "    max_norm=20)\n",
    "\n",
    "lr = .01\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "print_every = 10\n",
    "log_every = 1\n",
    "batch_size = 100\n",
    "\n",
    "losses = []\n",
    "separation_losses = []\n",
    "epoch_losses = [1e10]\n",
    "log.info('Beginning run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "separation avg loss: 9508530.0\n",
      "batch avg loss: 18647000.0\n",
      "separation avg loss: 9449642.0\n",
      "batch avg loss: 16040082.0\n",
      "separation avg loss: 9393456.0\n",
      "batch avg loss: 11498242.0\n",
      "separation avg loss: 9358736.0\n",
      "batch avg loss: 11477353.0\n",
      "separation avg loss: 9332779.0\n",
      "batch avg loss: 11275838.0\n",
      "separation avg loss: 9311316.0\n",
      "batch avg loss: 9880802.0\n",
      "separation avg loss: 9293846.0\n",
      "batch avg loss: 11183698.0\n",
      "separation avg loss: 9277876.0\n",
      "batch avg loss: 9029107.0\n",
      "separation avg loss: 9268210.0\n",
      "batch avg loss: 9696996.0\n",
      "separation avg loss: 9257793.0\n",
      "batch avg loss: 8290092.0\n",
      "Finished epoch 0 at Mon Jun 15 19:52:47 2020.\n",
      "best epoch so far!\n",
      "[10000000000.0, 10922262.0]\n",
      "separation avg loss: 9242815.0\n",
      "batch avg loss: 9671314.0\n",
      "separation avg loss: 9223214.0\n",
      "batch avg loss: 8989002.0\n",
      "separation avg loss: 9212239.0\n",
      "batch avg loss: 9116148.0\n",
      "separation avg loss: 9206072.0\n",
      "batch avg loss: 8248427.0\n",
      "separation avg loss: 9201946.0\n",
      "batch avg loss: 8064363.0\n",
      "separation avg loss: 9198073.0\n",
      "batch avg loss: 8348721.5\n",
      "separation avg loss: 9189750.0\n",
      "batch avg loss: 8548735.0\n",
      "separation avg loss: 9182417.0\n",
      "batch avg loss: 7920142.5\n",
      "separation avg loss: 9178713.0\n",
      "batch avg loss: 8252241.0\n",
      "separation avg loss: 9177770.0\n",
      "batch avg loss: 7891088.0\n",
      "Finished epoch 1 at Mon Jun 15 20:08:25 2020.\n",
      "best epoch so far!\n",
      "[10000000000.0, 10922262.0, 8205092.5]\n",
      "separation avg loss: 9176358.0\n",
      "batch avg loss: 6717352.0\n",
      "separation avg loss: 9175698.0\n",
      "batch avg loss: 7099662.5\n",
      "separation avg loss: 9173241.0\n",
      "batch avg loss: 7069904.0\n",
      "separation avg loss: 9167853.0\n",
      "batch avg loss: 7339442.5\n",
      "separation avg loss: 9163502.0\n",
      "batch avg loss: 7262079.0\n",
      "separation avg loss: 9156918.0\n",
      "batch avg loss: 6679362.0\n",
      "separation avg loss: 9150070.0\n",
      "batch avg loss: 7172015.0\n",
      "separation avg loss: 9143337.0\n",
      "batch avg loss: 6384460.5\n",
      "separation avg loss: 9137963.0\n",
      "batch avg loss: 6654080.0\n",
      "separation avg loss: 9129950.0\n",
      "batch avg loss: 7027306.5\n",
      "Finished epoch 2 at Mon Jun 15 20:23:39 2020.\n",
      "best epoch so far!\n",
      "[10000000000.0, 10922262.0, 8205092.5, 6934426.0]\n",
      "separation avg loss: 9130836.0\n",
      "batch avg loss: 6661112.5\n",
      "separation avg loss: 9133791.0\n",
      "batch avg loss: 6341170.5\n",
      "separation avg loss: 9129786.0\n",
      "batch avg loss: 6630347.5\n",
      "separation avg loss: 9121661.0\n",
      "batch avg loss: 6088875.0\n",
      "separation avg loss: 9112335.0\n",
      "batch avg loss: 6457315.5\n",
      "separation avg loss: 9103484.0\n",
      "batch avg loss: 5805209.0\n",
      "separation avg loss: 9097675.0\n",
      "batch avg loss: 5489625.5\n",
      "separation avg loss: 9092761.0\n",
      "batch avg loss: 6103947.0\n",
      "separation avg loss: 9087747.0\n",
      "batch avg loss: 6368539.0\n",
      "separation avg loss: 9083060.0\n",
      "batch avg loss: 5678168.0\n",
      "Finished epoch 3 at Mon Jun 15 20:39:01 2020.\n",
      "best epoch so far!\n",
      "[10000000000.0, 10922262.0, 8205092.5, 6934426.0, 6024185.5]\n",
      "separation avg loss: 9080744.0\n",
      "batch avg loss: 5142045.0\n",
      "separation avg loss: 9072998.0\n",
      "batch avg loss: 4966185.0\n",
      "separation avg loss: 9064850.0\n",
      "batch avg loss: 5536095.0\n",
      "separation avg loss: 9059049.0\n",
      "batch avg loss: 5307968.5\n",
      "separation avg loss: 9055777.0\n",
      "batch avg loss: 5619502.0\n",
      "separation avg loss: 9054045.0\n",
      "batch avg loss: 5283005.0\n",
      "separation avg loss: 9053714.0\n",
      "batch avg loss: 5491202.5\n",
      "separation avg loss: 9055496.0\n",
      "batch avg loss: 5191845.5\n",
      "separation avg loss: 9054586.0\n",
      "batch avg loss: 5344915.0\n",
      "separation avg loss: 9049162.0\n",
      "batch avg loss: 4866486.5\n",
      "Finished epoch 4 at Mon Jun 15 20:54:45 2020.\n",
      "best epoch so far!\n",
      "[10000000000.0, 10922262.0, 8205092.5, 6934426.0, 6024185.5, 5247213.5]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(len(train)).numpy()\n",
    "    nan_break = False\n",
    "    \n",
    "    for i in range(0,len(train), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch = train[indices]\n",
    "\n",
    "        model.train(); opt.zero_grad()\n",
    "\n",
    "        lhs = batch.values\n",
    "\n",
    "        l_batch, r_batch, neg_batch = model(lhs)\n",
    "        # nan tests...\n",
    "        l_test = np.isnan(np.mean(l_batch.detach().numpy()))\n",
    "        r_test = np.isnan(np.mean(r_batch.detach().numpy()))\n",
    "        neg_test = np.isnan(np.mean(neg_batch.detach().numpy()))\n",
    "        \n",
    "        if l_test or r_test or neg_test:\n",
    "            nan_break = True\n",
    "            break\n",
    "        \n",
    "        positive_similarity = torch.bmm(l_batch,r_batch.transpose(2,1)) #this is the same as dot product by row\n",
    "\n",
    "        negative_similarity = torch.bmm(l_batch, neg_batch.transpose(2,1)).squeeze(1)\n",
    "\n",
    "        star_loss = torch.sum(torch.clamp(.1 - positive_similarity + negative_similarity, min=0))\n",
    "        \n",
    "        # Now add in clustering loss for DOT categories\n",
    "        with torch.no_grad():\n",
    "            dot_emb = np.empty([dot.shape[0],100])\n",
    "            for j,doc in enumerate(dot):\n",
    "                s = doc.replace('\\t',' ').split(' ')\n",
    "                dot_emb[j] = model.embed_doc(s)\n",
    "            \n",
    "        separation_loss = torch.tensor(davies_bouldin_score(dot_emb,dot_labs) * 1000000)\n",
    "        separation_loss.requires_grad = True\n",
    "        \n",
    "        loss = star_loss + separation_loss\n",
    "        \n",
    "        losses.append(star_loss.detach().numpy())\n",
    "        separation_losses.append(separation_loss.detach().numpy())\n",
    "\n",
    "        loss.backward(); opt.step()\n",
    "        \n",
    "        if i % (print_every*batch_size) == 0:\n",
    "            print('separation avg loss: %s' % str(np.mean(separation_losses[(len(separation_losses)-10):])))\n",
    "            print('star avg loss: %s' % str(np.mean(losses[(len(losses)-10):])))\n",
    "        if i % (log_every*batch_size) == 0:\n",
    "            log.info('separation avg loss: %s' % str(np.mean(separation_losses[(len(separation_losses)-10):])))\n",
    "            log.info('star avg loss: %s' % str(np.mean(losses[(len(losses)-10):])))\n",
    "            \n",
    "    if nan_break:\n",
    "        print(\"you've got nans\")\n",
    "        log.warning(\"you've got nans\")\n",
    "        break\n",
    "    \n",
    "    print('Finished epoch %s at %s.' % (epoch,time.ctime()))\n",
    "    log.info(\"Finished epoch %s\" % str(epoch))\n",
    "    \n",
    "    epoch_loss = np.mean(losses[(len(losses)-100):])\n",
    "    \n",
    "    if epoch_loss < min(epoch_losses):\n",
    "        print('best epoch so far!')\n",
    "        log.info('best epoch so far!')\n",
    "        \n",
    "        weights = model.embeddings.weight\n",
    "        with open(OUTPUT_WEIGHTS + 'weights_best_epoch', 'wb') as f:\n",
    "            pickle.dump(weights.data.detach().numpy(), f)\n",
    "    \n",
    "    epoch_losses.append(epoch_loss)\n",
    "    print(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.embeddings.weight\n",
    "with open(OUTPUT_WEIGHTS + 'weights_%s' % SAMPLE_SIZE, 'wb') as f:\n",
    "    pickle.dump(weights.data.detach().numpy(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made it!\n"
     ]
    }
   ],
   "source": [
    "print('You made it!')\n",
    "log.info('You made it!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the weights to CSV\n",
    "# weights = model.input_embedding.weight\n",
    "# weights = weights.data.detach().numpy()\n",
    "# np.savetxt(\"weights_%s.csv\" % SAMPLE_SIZE, weights, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# val_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for j in range(0,len(valid), batch_size):\n",
    "#     valperm = torch.randperm(len(valid)).numpy()\n",
    "#     val_indices = valperm[j:j+batch_size]\n",
    "#     val_batch = valid[val_indices]\n",
    "\n",
    "#     val_lhs = val_batch.values\n",
    "\n",
    "#     val_l_batch, val_r_batch, val_neg_batch = model(val_lhs)\n",
    "\n",
    "#     val_positive_similarity = torch.bmm(val_l_batch,val_r_batch.transpose(2,1))\n",
    "#     val_negative_similarity = torch.bmm(val_l_batch, val_neg_batch.transpose(2,1)).squeeze(1)\n",
    "\n",
    "#     val_loss = torch.sum(torch.clamp(.1 - val_positive_similarity + val_negative_similarity, min=0))\n",
    "#     if j % (print_every*batch_size) == 0:\n",
    "#         print(val_loss)\n",
    "    \n",
    "#     val_accuracy_check = val_positive_similarity.squeeze(1) > val_negative_similarity[:,0].unsqueeze(1)\n",
    "#     val_acc += np.sum(val_accuracy_check.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(val_acc/len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# def build_vocab(train, min_ct = 2):\n",
    "#     ''' build vocabulary for an array/list/series of text '''\n",
    "#     # To do: smaller groups before aggregating to improve performance\n",
    "#     def wordcount_df(doc):\n",
    "#         tok = doc.split()\n",
    "#         d = pd.DataFrame.from_dict(Counter(tok),orient='index').reset_index().rename(columns={'index':'word'})\n",
    "#         return d\n",
    "\n",
    "#     d_list = [wordcount_df(x) for x in train]\n",
    "\n",
    "#     d = pd.concat(d_list,axis=0)\n",
    "\n",
    "#     d = d.groupby(['word'])[0].sum().sort_values(ascending=False)\n",
    "#     d = d[d >= min_ct]\n",
    "    \n",
    "#     d = dict(zip(d.index.values, range(len(d))))\n",
    "    \n",
    "#     return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = train[0:100]\n",
    "# validation = train[100:150]\n",
    "# batch_size = 100\n",
    "\n",
    "# model.train()\n",
    "# opt.zero_grad()\n",
    "\n",
    "# lhs = batch.values\n",
    "\n",
    "# l_batch, r_batch, neg_batch = model(lhs)\n",
    "\n",
    "# positive_similarity = torch.bmm(l_batch,r_batch.transpose(2,1)) #this is the same as dot product by row\n",
    "\n",
    "# negative_similarity = torch.bmm(l_batch, neg_batch.transpose(2,1)).squeeze(1)\n",
    "\n",
    "# loss = torch.mean(torch.clamp(.1 - positive_similarity + negative_similarity, min=0))\n",
    "# loss\n",
    "\n",
    "# loss.backward(); opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy of predictions in current batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_check = positive_similarity.squeeze(1) > negative_similarity[:,0].unsqueeze(1)\n",
    "# acc = np.mean(similarity_check.detach().numpy())\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def embed_doc(d,vocab,embedding,normalize=False):\n",
    "#     positions = []\n",
    "#     for t in d:\n",
    "#         try:\n",
    "#             positions.append(vocab[t])\n",
    "#         except KeyError:\n",
    "#             pass\n",
    "#     output = torch.sum(embedding(torch.LongTensor(positions)),dim=0)\n",
    "#     if normalize:\n",
    "#         output = output / output.norm()\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # similarity\n",
    "# l_batch = []\n",
    "# r_batch = []\n",
    "# neg_batch = []\n",
    "\n",
    "# for i in range(len(batch)):\n",
    "#     #Positive similarity\n",
    "#     s = batch.values[i].split('\\t') #sentences\n",
    "#     if type(s) == str: #only one sentence in s\n",
    "#         a = s\n",
    "#         b = s\n",
    "#     else:\n",
    "#         a, b = np.random.choice(s, 2, False)\n",
    "    \n",
    "#     a = a.split()\n",
    "#     b = b.split()\n",
    "    \n",
    "#     a_emb = embed_doc(a,train_vocab,input_embedding,normalize=True)\n",
    "#     b_emb = embed_doc(b,train_vocab,input_embedding,normalize=True)\n",
    "    \n",
    "#     l_batch.append(a_emb)\n",
    "#     r_batch.append(b_emb)\n",
    "\n",
    "#     #Negative similarity\n",
    "#     negs = []\n",
    "#     for _i in range(k * 3):\n",
    "#         index = np.random.choice(len(batch))\n",
    "#         if not index == i: #if it's not from the same document\n",
    "#             c = batch.values[index].split('\\t')\n",
    "#             c = np.random.choice(c, 1)[0].split()\n",
    "#             c_emb = embed_doc(c,train_vocab,input_embedding,normalize=True)\n",
    "#             negs.append(c_emb)\n",
    "#             if(len(negs) >= k):\n",
    "#                 break\n",
    "    \n",
    "#     neg_batch.append(torch.stack(negs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
