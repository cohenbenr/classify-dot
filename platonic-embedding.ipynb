{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! pip install --quiet seaborn toolz fuzzywuzzy\n",
    "! pip install --quiet diskcache python-Levenshtein lightgbm lime\n",
    "! pip install --quiet adabound\n",
    "! pip install -e 'git://github.com/nandanrao/embed-software.git#egg=embed_software'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! pip install --quiet statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from validation.data import dot_train_data, get_soc_n, get_dictionary, indeed_test_data, virginia_test_data\n",
    "from embed_software.preprocess import *\n",
    "from embed_software.utils import get_embeddings, embed_docs\n",
    "from classification.embedding import PreEmbeddedVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pd.set_option('max_colwidth',50)\n",
    "pd.set_option('display.width', 700)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SOC_LEVEL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = dot_train_data(SOC_LEVEL, include_tasks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def make_taskgroup(dx, dy):\n",
    "    return (pd.DataFrame({'soc': dy, 'content': dx})\n",
    "            .groupby('soc')\n",
    "            .apply(lambda df: reduce(lambda a,b: a + b, df.content.sample(frac=0.33)))\n",
    "            .reset_index()\n",
    "            .rename(columns= {0: 'content'}))\n",
    "\n",
    "dx, dy = dot_train_data(6, include_tasks=True, include_dot=False)        \n",
    "tasks = pd.concat([make_taskgroup(dx, dy) for _ in range(6)])\n",
    "\n",
    "tasks['soc'] = get_soc_n(tasks.soc.map(str), 3)\n",
    "\n",
    "y_train, X_train = pd.concat([y_train, tasks.soc]).reset_index(drop=True), pd.concat([X_train, tasks.content]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "not_production = y_train != 519\n",
    "\n",
    "X_train, y_train = X_train[not_production], y_train[not_production]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test, va_df = virginia_test_data('../data/va_job_posts.json', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def _embed(embedding, d, sentences):\n",
    "    if sentences == True:\n",
    "        fn = embedding.embed_paragraph\n",
    "    else:\n",
    "        fn = embedding.embed_doc\n",
    "\n",
    "    doc = fn(d).T.reshape(1, 100, -1)\n",
    "    return torch.from_numpy(doc).float()\n",
    "\n",
    "def load_source(embedding, X_train, y_train, sentences):\n",
    "    for d,y in zip(X_train, y_train):\n",
    "        doc = _embed(embedding, d, sentences)\n",
    "        label = torch.tensor([y]).long()\n",
    "        yield doc, label\n",
    "\n",
    "def load_target(embedding, docs, sentences):\n",
    "    for d in docs:\n",
    "        yield _embed(embedding, d, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from classification.embedding import Embedding\n",
    "\n",
    "embedding = Embedding('../glove-models/glove-va-100.txt', sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "label_lookup = {v:k for k,v in pd.Series(y_train.unique()).to_dict().items()}\n",
    "y_train_idx = [label_lookup[y] for y in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "docs, labels = zip(*list(load_source(embedding, X_train, y_train_idx, sentences = False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "idx = np.random.choice(X_test.index, 50000, replace=False)\n",
    "idx = [i for i in idx if X_test[i] is not None]\n",
    "\n",
    "target = list(load_target(embedding, X_test[idx], sentences = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# w = 1 / pd.Series(np.array([l.to(device='cpu').numpy() for l in labels]).reshape(-1)).value_counts().reset_index().sort_values('index')[0].values\n",
    "\n",
    "def make_weights(y_train):\n",
    "    v = (1 / y_train.value_counts())\n",
    "    v = (v / v.sum())*v.shape[0]\n",
    "    w_df = v.reset_index().rename(columns={'soc': 'count', 'index': 'soc'})\n",
    "    w = pd.DataFrame(y_train).merge(w_df, how='left')['count'].values\n",
    "    return torch.from_numpy(w).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "weights = make_weights(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from toolz import curry\n",
    "from time import perf_counter\n",
    "from math import ceil\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, net, opt, device, criterion = None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.net = net.to(device=device)\n",
    "        self.opt = opt(net)\n",
    "        self.criterion = criterion\n",
    "        self.net.register_backward_hook(printgradnorm)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X).view(-1)\n",
    "\n",
    "    def evaluate(self, source, target, label, targets=None):\n",
    "        out = self.__call__(source)\n",
    "        loss = self.criterion(out.reshape(1, -1), label)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Discriminator(Classifier):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dummy_source = torch.tensor([1.]).to(device=self.device)\n",
    "        self.dummy_target = torch.tensor([0.]).to(device=self.device)\n",
    "        \n",
    "    def evaluate(self, source, target, label):\n",
    "        guess_s = self.__call__(source)\n",
    "        guess_t = self.__call__(target)\n",
    "        loss = self.criterion(guess_s, self.dummy_source)\n",
    "        loss += self.criterion(guess_t, self.dummy_target)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class MockOpt():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self):\n",
    "        return\n",
    "\n",
    "    def step(self):\n",
    "        return\n",
    "\n",
    "\n",
    "class Distancer(Classifier):\n",
    "    def __init__(self, alpha, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.opt = MockOpt()\n",
    "\n",
    "    def evaluate(self, source, target, label, targets):\n",
    "        M = torch.cdist(torch.stack([target]), torch.stack(targets))\n",
    "        return self.alpha * M.sum() / target.sum() \n",
    "        \n",
    "\n",
    "class PlatonicNet():\n",
    "    def __init__(self, embedder, classifier, discriminator, batch_size=64, n_epochs=5, grad_norm_clip=0.25, discriminator_mix= -1.0):\n",
    "        self.discriminator = discriminator\n",
    "        self.classifier = classifier\n",
    "        self.embedder = embedder\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.grad_norm_clip = grad_norm_clip\n",
    "        self.disc_mix = discriminator_mix\n",
    "\n",
    "    def load_data(self, docs, labels, weights, target):\n",
    "        self.docs, self.labels, self.weights, self.target = docs, labels, weights, target.copy()\n",
    "\n",
    "    def batch(self, size):\n",
    "        random.shuffle(self.target)\n",
    "\n",
    "        dat = list(zip(self.docs, self.labels, self.weights, self.target))\n",
    "        random.shuffle(dat)\n",
    "\n",
    "        out = []\n",
    "        while dat:\n",
    "            head,dat = dat[:size], dat[size:]\n",
    "            out.append(head)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def epoch(self, embedder):\n",
    "        epoch_disc_loss = 0\n",
    "        epoch_class_loss = 0\n",
    "        epoch_start = perf_counter()\n",
    "\n",
    "        for i,batch in enumerate(self.batch(self.batch_size)):\n",
    "            batch_disc_loss = 0\n",
    "            batch_class_loss = 0\n",
    "\n",
    "            # run for each net, classifier and discriminator\n",
    "            # (self.discriminator, self.disc_mix)\n",
    "            for net,sign in [(self.classifier, 1.)]:\n",
    "\n",
    "                # due to pytorch updating, \n",
    "                # run twice, once for embedder, once for the other model\n",
    "                for updating_model,sgn in [(embedder, sign), (net, 1.)]:\n",
    "                    updating_model.opt.zero_grad()\n",
    "                    loss = 0\n",
    "\n",
    "                    sources, labels, weights, targets = zip(*batch)                    \n",
    "                    sources = [embedder(s) for s in sources]\n",
    "                    targets = [embedder(t) for t in targets]\n",
    "                    \n",
    "                    b = zip(sources, labels, weights, targets)\n",
    "                    for source, label, weight, target in b:\n",
    "                        # l = net.evaluate(embedder(source), embedder(target), label)\n",
    "                        l = net.evaluate(source, target, label, targets)\n",
    "                        loss += l*weight\n",
    "\n",
    "                    # Flip the loss for embedding/discriminator\n",
    "                    loss *= sgn\n",
    "\n",
    "                    if torch.isnan(loss):\n",
    "                        raise Exception('LOSS/EMBEDDING IS NAN')\n",
    "\n",
    "                    # Update loss records for printing\n",
    "                    if updating_model == self.discriminator:\n",
    "                        batch_disc_loss += loss\n",
    "                        epoch_disc_loss += loss\n",
    "                    elif updating_model == self.classifier:\n",
    "                        batch_class_loss += loss\n",
    "                        epoch_class_loss += loss\n",
    "\n",
    "                    # optimize\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_value_(updating_model.net.parameters(), self.grad_norm_clip)\n",
    "                    updating_model.opt.step()\n",
    "                    \n",
    "            # if i % 100 == 0:\n",
    "                # print(f'Batch class/disc loss: {batch_class_loss} ::: {batch_disc_loss}')\n",
    "        epoch_time = round((perf_counter() - epoch_start)/60)\n",
    "        print(f'----------- EPOCH --------------\\nEpoch finished in {epoch_time} minutes. class/disc loss: {epoch_class_loss} ::: {epoch_disc_loss}')        \n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self.epoch(self.embedder)            \n",
    "\n",
    "\n",
    "def printgradnorm(self, grad_input, grad_output):\n",
    "    pass\n",
    "    # if grad_input[0].norm() > 200000.:\n",
    "        # print('grad_input norm:', grad_input[0].norm())\n",
    "\n",
    "class GatedNet(torch.nn.Module):\n",
    "    def __init__(self, embed_size, layers):\n",
    "        super().__init__()\n",
    "        self.conver = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_size, out_channels=layers, kernel_size=1, groups=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.conver.register_backward_hook(printgradnorm)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        convs = self.conver(x)\n",
    "        out = torch.matmul(x, torch.t(convs.max(1).values))\n",
    "        return out / torch.norm(out)  \n",
    "\n",
    "class ParallelFilters(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "        for i,net in enumerate(filters):\n",
    "            self.add_module(f'filter_{i}', net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([net(x) for net in self.children()], 1)    \n",
    "\n",
    "\n",
    "class NormedSum(nn.Module):\n",
    "    def forward(self, x):\n",
    "        y = x.sum(2).reshape(1,-1,1)\n",
    "        return y / torch.norm(y)\n",
    "\n",
    "def _embedder(embed_size, layers, normed_sum = False, dropout = 0.5):\n",
    "    filters = [\n",
    "        nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_size, out_channels=out_channels, kernel_size=kernel_size, groups=1, padding=kernel_size - 1),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1),\n",
    "            nn.Dropout(p=dropout))\n",
    "        for kernel_size,out_channels in layers]\n",
    "\n",
    "    if normed_sum:\n",
    "        filters = [NormedSum()] + filters\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        ParallelFilters(filters)\n",
    "    )\n",
    "\n",
    "    net.register_backward_hook(printgradnorm)\n",
    "    return net\n",
    "\n",
    "def _embedder_single(embed_size, out_channels):\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=embed_size, out_channels=out_channels, kernel_size=1, groups=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.AdaptiveMaxPool1d(output_size=1),\n",
    "        nn.Dropout(p=0.4)\n",
    "    )\n",
    "\n",
    "    net.register_backward_hook(printgradnorm)\n",
    "    return net    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from adabound import AdaBound\n",
    "\n",
    "@curry\n",
    "def adam_opt(lr, net):\n",
    "    return optim.Adam(net.parameters(), lr=lr, weight_decay=1.0)\n",
    "\n",
    "@curry\n",
    "def ab_opt(lr, wd, net):\n",
    "    return AdaBound(net.parameters(), lr=lr, final_lr=0.01, weight_decay=wd)\n",
    "\n",
    "\n",
    "def get_size(filters, normed_sum):\n",
    "    s = np.sum([f[1] for f in filters])\n",
    "    if normed_sum:\n",
    "        s += 100\n",
    "    return s\n",
    "\n",
    "device = torch.device('cuda')\n",
    "n_classes = y_train.unique().shape[0]\n",
    "filters = [(1, 50)]\n",
    "normed_sum = True\n",
    "\n",
    "final_layer_size = get_size(filters, normed_sum)\n",
    "\n",
    "print(final_layer_size)\n",
    "\n",
    "embedder = Classifier(_embedder(100, filters, normed_sum=normed_sum, dropout=0.5), \n",
    "                      ab_opt(0.00002, 0.0),\n",
    "                      device)\n",
    "\n",
    "classifier = Classifier(nn.Sequential(nn.Linear(final_layer_size, n_classes)),\n",
    "                        ab_opt(0.0001, 3.0), \n",
    "                        device,\n",
    "                        nn.CrossEntropyLoss())\n",
    "\n",
    "discriminator = Discriminator(nn.Sequential(nn.Linear(final_layer_size, 1)), \n",
    "                              ab_opt(0.0001, 1.0), \n",
    "                              device, \n",
    "                              nn.BCEWithLogitsLoss())\n",
    "\n",
    "# discriminator = Distancer(1, \n",
    "#                           nn.Linear(final_layer_size, 1), \n",
    "#                           ab_opt(0.0001, 1.0), \n",
    "#                           device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "move_to_device = lambda x: [i.to(device=device) for i in x]\n",
    "\n",
    "docs = move_to_device(docs)\n",
    "labels = move_to_device(labels)\n",
    "target = move_to_device(target)\n",
    "weights = move_to_device(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = PlatonicNet(embedder, classifier, discriminator, n_epochs=25, grad_norm_clip=0.1, discriminator_mix= -0.05)\n",
    "model.load_data(docs, labels, weights, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'model-09-11-a.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "i = 90\n",
    "\n",
    "def get_spread(d):\n",
    "    vals = model.embedder.net.conver(d).max(1).values.detach().numpy()\n",
    "    return vals.max() - vals.min()\n",
    "    \n",
    "\n",
    "np.mean([get_spread(d) for d in docs[:500]]), np.mean([get_spread(d) for d in target[:500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "i = 44\n",
    "\n",
    "idx = np.where(model.embedder.net.conver(target[i]).max(1).values.detach().numpy() < .4)[1]\n",
    "np.array(df.content.iloc[i].split('\\t'))[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from validation.scoring import bubbleup_score\n",
    "from validation.scoring import BubbleUpMixin\n",
    "\n",
    "class BubbleUpLogisticRegression(BubbleUpMixin, LogisticRegression):\n",
    "    pass\n",
    "\n",
    "def simple_embed(doc):\n",
    "    X = doc.sum(2).reshape(-1)\n",
    "    return X / torch.norm(X)\n",
    "\n",
    "def ss_embed(doc):\n",
    "    d = embedding.embed_doc(doc).sum(0)\n",
    "    return d / np.linalg.norm(d)\n",
    "\n",
    "def _listify(a):\n",
    "    if not hasattr(a, '__len__'):\n",
    "        return [a]\n",
    "    return a\n",
    "\n",
    "def _is_hit(ys, preds):\n",
    "    ys, preds = _listify(ys), _listify(preds)\n",
    "    return len(set(ys) & set(preds)) > 0\n",
    "\n",
    "def multi_score(y_test, preds):\n",
    "    hits = [_is_hit(y, p) for y,p in zip(y_test, preds)] \n",
    "    return np.sum(hits) / len(hits)\n",
    "\n",
    "get_soc_n_str = lambda x: ''.join(x.strip().split('.')[0].split('-'))[:3]\n",
    "\n",
    "get_all_possibilities = lambda y: set([get_soc_n_str(i) for i in y])\n",
    "\n",
    "y_possibilities = [[r.onet_soc_code] + r.occupationalCategory.split(',') for i,r in va_df.loc[idx, :].iterrows()]\n",
    "y_possibilities = [get_all_possibilities(y) for y in y_possibilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5265022137887413"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xe_train = [simple_embed(d).detach().cpu().numpy() for d in docs]\n",
    "\n",
    "clf = BubbleUpLogisticRegression(C=1., n_jobs=-1, solver='lbfgs', multi_class='multinomial').set_bubbles(soc_n=3, top_x=1)\n",
    "\n",
    "clf.fit(Xe_train, y_train)\n",
    "preds = clf.predict(Xe_train)\n",
    "\n",
    "multi_score(y_train, [[int(p) for p in pred] for pred in preds])\n",
    "\n",
    "# Just DOT, Soc 3\n",
    "# 0.567\n",
    "\n",
    "# Just DOT, no 519\n",
    "# 0.527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31368"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xe_test = [simple_embed(d).detach().cpu().numpy() for d in target]\n",
    "preds = clf.predict(Xe_test)\n",
    "multi_score(y_possibilities, preds)\n",
    "\n",
    "\n",
    "# Just DOT, Soc 3\n",
    "# 0.307\n",
    "\n",
    "\n",
    "# Just DOT, no 519\n",
    "# 0.312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "confusion_matrix(y_train, preds)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43605313092979125"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xp_train = [model.embedder(d).detach().cpu().numpy() for d in docs]\n",
    "\n",
    "clf = BubbleUpLogisticRegression(C=1., n_jobs=-1, solver='lbfgs', multi_class='multinomial').set_bubbles(soc_n=3, top_x=1)\n",
    "\n",
    "clf.fit(Xp_train, y_train)\n",
    "preds = clf.predict(Xp_train)\n",
    "\n",
    "multi_score(get_soc_n(y_train.map(str), 3), [[int(p) for p in pred] for pred in preds])\n",
    "\n",
    "# ------ 0.39 baseline\n",
    "\n",
    "\n",
    "# 0.5102\n",
    "# 0.54275\n",
    "# 0.5583\n",
    "\n",
    "# --------- adversarial, SOC 6, bubbleup 3\n",
    "# 0.3785\n",
    "# 0.3887\n",
    "\n",
    "# --------- adversarial, SOC 3, only dict\n",
    "\n",
    "# 0.745 - a\n",
    "# 0.786 - b\n",
    "\n",
    "# --------- adversarial, SOC 3, weighted (9-03)\n",
    "# 0.627 - a\n",
    "# 0.657 - b\n",
    "\n",
    "# --------- adversarial, SOC 3, weighted, no 519 (9-03)\n",
    "# 0.850 - c\n",
    "# 0.893 - d\n",
    "\n",
    "# --------- non-adversarial, SOC 3, weighted, no 519, sigmoid\n",
    "# 0.611 - 0.535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08214"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xp_test = [model.embedder(d).detach().cpu().numpy() for d in target]\n",
    "preds = clf.predict(Xp_test)\n",
    "multi_score(y_possibilities, preds)\n",
    "\n",
    "# ------ 0.07 baseline\n",
    "\n",
    "# 0.19544 - c\n",
    "# 0.16636 - b\n",
    "# 0.19374 - a\n",
    "\n",
    "# -------- adversarial, SOC 6, bubbleup 3\n",
    "# 0.14124\n",
    "# 0.13644\n",
    "\n",
    "# --------- adversarial, SOC 3, only dict\n",
    "# 0.05\n",
    "# 0.05\n",
    "\n",
    "# -------- adversarial, SOC 3, weighted (9-03)\n",
    "# 0.1013 - a \n",
    "# 0.1084 - b\n",
    "\n",
    "# --------- adversarial, SOC 3, weighted, no 519 (9-03)\n",
    "# 0.1597 - c\n",
    "# 0.1520 - d\n",
    "\n",
    "\n",
    "# -------- non-adversarial, SOC 3, weighted, no 519, sigmoid\n",
    "# 0.15 - 0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def interpret(model, target, X, ind):\n",
    "    vals, indices = list(model.embedder.net[0].children())[1][0](target[ind]).max(2)\n",
    "    numpize = lambda t: t.to(device='cpu').detach().numpy().reshape(-1) \n",
    "    vals, indices = numpize(vals), numpize(indices)\n",
    "    _, words = embedding.embed_doc(X[ind], return_words=True)\n",
    "    \n",
    "#     vals, indices = zip(*sorted(list(zip(vals, indices)), key = lambda x: x[0], reverse=True))\n",
    "    w = np.array(words)[list(indices)]\n",
    "    return pd.DataFrame((w, vals)).T.rename(columns={0: 'word', 1: 'value'})\n",
    "\n",
    "    \n",
    "\n",
    "interpret(model, target, X_test[idx].reset_index(drop=True), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%notebook -e platonic-embedding-output-9-02.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341305            {, 291}\n",
       "68059          {413, 414}\n",
       "316045              {132}\n",
       "639904    {119, 292, 291}\n",
       "405625         {151, 271}\n",
       "               ...       \n",
       "430509            {333, }\n",
       "596229         {412, 414}\n",
       "123363    {435, 999, 412}\n",
       "437148            {, 999}\n",
       "412151         {353, 412}\n",
       "Length: 50000, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_possibilities)[target_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE=100000\n",
    "X_test, y_test, ids = indeed_test_data('../data/us/everything.csv', SAMPLE_SIZE, 6)\n",
    "X_train, y_train = dot_train_data(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Xe_test = [model.embedder(d).detach().numpy() for d in load_target(X_test)]\n",
    "Xe_train = [model.embedder(d).detach().numpy() for d in load_target(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=-1, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=5., n_jobs=-1, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "clf.fit(Xe_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46860896376066846"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bubbleup_score(y_train, Xe_test, y_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Xe_test = [simple_embed(d).detach().numpy() for d in load_target(X_test)]\n",
    "Xe_train = [simple_embed(d).detach().numpy() for d in load_target(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=-1, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=5., n_jobs=-1, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(Xe_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4752223066267483"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bubbleup_score(y_train, Xe_test, y_test, clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "platonic-embedding.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
