{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Embedding():\n",
    "    def __init__(self, path):\n",
    "        embedding = pd.read_csv(path, sep='\\t', header=None)\n",
    "        keys = embedding.iloc[:,0]\n",
    "        vals = embedding.iloc[:,1:].values\n",
    "        self.lookup = {k:v for k,v in zip(keys, vals)}\n",
    "\n",
    "    def embed_doc(self, doc, return_words = False):\n",
    "        words = []\n",
    "        vecs = []\n",
    "        for word in doc.split():\n",
    "            try:\n",
    "                vecs.append(self.lookup[word])\n",
    "                words.append(word)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if not return_words: \n",
    "            return np.array(vecs)\n",
    "        return np.array(vecs), words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "embedding = Embedding('../indeed-embeds/model.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from gcsfs import GCSFileSystem\n",
    "\n",
    "fs = GCSFileSystem(project='labor-market-data')\n",
    "with fs.open('lmd-classify-dot/data/us/company-everything.csv') as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "df['title'] = df.title.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "KEY_A = 'category'\n",
    "\n",
    "overlapping = (df.sort_values(KEY_A)\n",
    "               .groupby(KEY_A)\n",
    "               .filter(lambda df: df.shape[0] > 15 and df.company.unique().shape[0] > 5)\n",
    "               .groupby('company')\n",
    "               .filter(lambda df: df.shape[0] > 15 and df[KEY_A].unique().shape[0] > 5)\n",
    "               .groupby(KEY_A)\n",
    "               .filter(lambda df: df.shape[0] > 10 and df.company.unique().shape[0] > 3)\n",
    "               .groupby('company')\n",
    "               .filter(lambda df: df.shape[0] > 10 and df[KEY_A].unique().shape[0] > 3))\n",
    "\n",
    "top_companies = overlapping.company.value_counts().index.values[:10]\n",
    "top_titles = overlapping[KEY_A].value_counts().index.values[:10]\n",
    "overlapping = overlapping[(overlapping[KEY_A].isin(top_titles)) & (overlapping.company.isin(top_companies))]\n",
    "\n",
    "overlapping = (overlapping\n",
    "               .merge(pd.DataFrame(list(enumerate(overlapping[KEY_A].unique())), columns = ['title_class', KEY_A]),\n",
    "                      how = 'left', on=KEY_A)\n",
    "               .merge(pd.DataFrame(list(enumerate(overlapping.company.unique())), columns = ['company_class', 'company']),\n",
    "                      how = 'left', on='company'))[['company_class', KEY_A, 'content', 'company', 'title_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    for i,d in df.iterrows():\n",
    "        doc = embedding.embed_doc(d.content.lower()).T.reshape(1, 100, -1)\n",
    "        doc = torch.from_numpy(doc).float()\n",
    "        class_distinct, class_ignore = torch.from_numpy(np.array(d.title_class)).long(), torch.from_numpy(np.array(d.company_class)).long()\n",
    "        yield doc, [class_distinct, class_ignore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "docs, labels = zip(*load_data(overlapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from toolz import curry\n",
    "import attr\n",
    "import random\n",
    "\n",
    "class Classifier():\n",
    "    def __init__(self, net, opt, criterion = None):\n",
    "        self.net = net\n",
    "        self.opt = opt(net)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.net(X).view(-1)\n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, Embedder, Classifier, Discriminator, aspects, batch_size=64, n_epochs=5):\n",
    "        self.Discriminator = Discriminator\n",
    "        self.Classifier = Classifier\n",
    "        self.embedders = [Embedder() for _ in range(aspects)]\n",
    "        self.aspects = aspects\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "\n",
    "    def load_data(self, docs, labels):\n",
    "        self.docs, self.labels = docs, labels\n",
    "\n",
    "    def batch(self, docs, labels, size):\n",
    "        dat = list(zip(docs, labels))\n",
    "        random.shuffle(dat)\n",
    "        out = []\n",
    "        while dat:\n",
    "            head,dat = dat[:size], dat[size:]\n",
    "            out.append(head)\n",
    "        return out\n",
    "\n",
    "    def epoch(self, embedder, nets, aspect):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in self.batch(self.docs, self.labels, self.batch_size):\n",
    "\n",
    "            # run for each aspect, classifier and discriminators\n",
    "            for net,sign in nets:\n",
    "\n",
    "                # due to pytorch updating, \n",
    "                # run twice, once for embedder, once for the other model\n",
    "                for updating_model,sgn in [(embedder, sign), (net, 1)]:\n",
    "                    updating_model.opt.zero_grad()\n",
    "                    for doc,labels in batch:\n",
    "                        label = labels[aspect].reshape(1)\n",
    "                        doc_em = embedder(doc)\n",
    "\n",
    "                        # embed title\n",
    "                        out = net(doc_em).reshape(1, -1)\n",
    "\n",
    "                        # pass title embed and doc embed to criterion\n",
    "                        loss = net.criterion(out, label)\n",
    "                        loss *= sign\n",
    "                        loss.backward()\n",
    "                        epoch_loss += loss\n",
    "                    updating_model.opt.step()\n",
    "        print(epoch_loss)\n",
    "\n",
    "    def train(self):\n",
    "        for aspect,embedder in enumerate(self.embedders):\n",
    "            classifier = self.Classifier()\n",
    "            discriminators = [self.Discriminator() for i in range(self.aspects - 1)]\n",
    "\n",
    "            # insert classifier \n",
    "            nets = [(d, -1) for d in discriminators]\n",
    "            nets.insert(aspect, (classifier, 1))\n",
    "\n",
    "            for epoch in range(self.n_epochs):\n",
    "                self.epoch(embedder, nets, aspect)            \n",
    "\n",
    "class GatedNet(torch.nn.Module):\n",
    "    def __init__(self, embed_size, layers):\n",
    "        super().__init__()\n",
    "        self.conver = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_size, out_channels=layers, kernel_size=1, groups=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):        \n",
    "        convs = self.conver(x)\n",
    "        out = torch.matmul(x, torch.t(convs.max(1).values))\n",
    "        return out / torch.norm(out)\n",
    "\n",
    "\n",
    "def _gated_embedder(embed_size, layers):\n",
    "    return GatedNet(embed_size, layers)\n",
    "\n",
    "def _embedder(embed_size, layers):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels=embed_size, out_channels=layers, kernel_size=1, groups=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.AdaptiveMaxPool1d(output_size=1),\n",
    "        nn.Dropout(p=0.25)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-28623.2852, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-105733.5703, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-200111.4844, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-296963.7812, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-391554.0625, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-484005.4375, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-574920.7500, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-664975.5625, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-754278.3125, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-843107.7500, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-47610.7695, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-150088.4062, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-270993.6875, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-391928.2812, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-509017.8750, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-624099.1250, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-738028.1875, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-850889.8125, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-962848.8125, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1074132.7500, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "logistic_regression = lambda P: nn.Sequential(nn.Linear(P, 10))\n",
    "sgd = lambda net: optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "classifier = lambda: Classifier(logistic_regression(100), sgd, criterion)\n",
    "discriminator = lambda: Classifier(logistic_regression(100), lambda net: optim.Adam(net.parameters(), lr=0.01), criterion)\n",
    "embedder = lambda: Classifier(_gated_embedder(100, 40), sgd)\n",
    "\n",
    "model = Model(embedder, classifier, discriminator, 2, n_epochs=10)\n",
    "model.load_data(docs, labels)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add = pd.DataFrame(list(zip(overlapping.title.unique(), [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0])), columns = ['title', 'soft'])\n",
    "# oo = overlapping.merge(add, how='left', on='title')\n",
    "\n",
    "sns.scatterplot(x = 'x', y = 'y', hue='label', data = pd.DataFrame(MDS(2).fit_transform(X.astype(np.float64)), columns = ['x', 'y']).assign(label = overlapping.category))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "name": "disentangled-embedding.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
