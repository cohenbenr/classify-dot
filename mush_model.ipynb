{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -e 'git://github.com/nandanrao/embed-software.git#egg=embed_software'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gcsfs import GCSFileSystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "#import torchtext\n",
    "\n",
    "from validation.data import *\n",
    "\n",
    "from src.model import StarSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(fmt=\"%(asctime)s %(levelname)s: %(message)s\", \n",
    "                          datefmt=\"%Y-%m-%d - %H:%M:%S\")\n",
    "fh = logging.FileHandler(\"mush_model.log\", \"w\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 40000\n",
    "SOC_LEVEL = 2\n",
    "BUBBLE_UP = 2\n",
    "OUTPUT_WEIGHTS = 'data/mush/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('Pulling Indeed data for sample size %s' % SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job ads data\n",
    "indeed = get_indeed_texts('../data/us/everything.csv',use_gcs=True,nrows=SAMPLE_SIZE)\n",
    "indeed = indeed['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32284,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get DOT classifications data\n",
    "dot, _ = dot_train_data(SOC_LEVEL)\n",
    "dot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([indeed,dot],axis=0)\n",
    "train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('About to train vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer(min_df = 5,\n",
    "                             max_df = .95)\n",
    "Vectorizer.fit(train)\n",
    "\n",
    "train_vocab = Vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the file\n",
    "with open(OUTPUT_WEIGHTS + 'train_vocab_%s' % SAMPLE_SIZE, 'wb') as f:\n",
    "    pickle.dump(train_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26940"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(OUTPUT_WEIGHTS + \"train_vocab_%s\" % SAMPLE_SIZE,\"rb\") as f:\n",
    "#     train_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('Trained Vocab of size %s' % str(len(train_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StarSpace(\n",
    "    d_embed=100,\n",
    "    vocabulary=train_vocab,\n",
    "    k_neg = 10,\n",
    "    max_norm=20)\n",
    "\n",
    "lr = .01\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "print_every = 100\n",
    "log_every = 1\n",
    "batch_size = 100\n",
    "\n",
    "losses = []\n",
    "epoch_losses = [1e10]\n",
    "log.info('Beginning run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch avg loss: 19154798.0\n",
      "batch avg loss: 6168423.5\n",
      "batch avg loss: 5223069.0\n",
      "batch avg loss: 4599096.0\n",
      "batch avg loss: 4223045.0\n",
      "batch avg loss: 3898170.0\n",
      "batch avg loss: 3443390.8\n",
      "batch avg loss: 3278955.5\n",
      "Finished epoch 0 at Mon Jun 15 12:25:15 2020.\n",
      "best epoch so far!\n",
      "[10000000000.0, 3374087.2]\n",
      "batch avg loss: 3374129.5\n",
      "batch avg loss: 3128548.8\n",
      "batch avg loss: 2837095.2\n",
      "batch avg loss: 2817449.8\n",
      "batch avg loss: 2844337.5\n",
      "batch avg loss: 2416164.5\n",
      "batch avg loss: 2419067.2\n",
      "batch avg loss: 2640782.5\n",
      "Finished epoch 1 at Mon Jun 15 12:33:41 2020.\n",
      "best epoch so far!\n",
      "[10000000000.0, 3374087.2, 2505417.8]\n",
      "batch avg loss: 2346416.5\n",
      "batch avg loss: 2164786.0\n",
      "batch avg loss: 2085329.6\n",
      "batch avg loss: 2199813.8\n",
      "batch avg loss: 2017609.2\n",
      "batch avg loss: 1818206.8\n",
      "batch avg loss: 2145155.2\n",
      "batch avg loss: 2038778.8\n",
      "Finished epoch 2 at Mon Jun 15 12:42:12 2020.\n",
      "best epoch so far!\n",
      "[10000000000.0, 3374087.2, 2505417.8, 1960913.6]\n",
      "batch avg loss: 1722633.2\n",
      "batch avg loss: 1885241.0\n",
      "batch avg loss: 1771172.6\n",
      "batch avg loss: 1872810.0\n",
      "batch avg loss: 1718904.0\n",
      "batch avg loss: 1792808.4\n",
      "batch avg loss: 1631900.2\n",
      "batch avg loss: 1578883.2\n",
      "Finished epoch 3 at Mon Jun 15 12:50:50 2020.\n",
      "best epoch so far!\n",
      "[10000000000.0, 3374087.2, 2505417.8, 1960913.6, 1651534.1]\n",
      "batch avg loss: 1680874.6\n",
      "batch avg loss: 1472712.2\n",
      "batch avg loss: 1612932.6\n",
      "batch avg loss: 1424697.8\n",
      "batch avg loss: 1373509.1\n",
      "batch avg loss: 1541370.2\n",
      "batch avg loss: 1485549.9\n",
      "batch avg loss: 1439975.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(len(train)).numpy()\n",
    "    nan_break = False\n",
    "    \n",
    "    for i in range(0,len(train), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch = train[indices]\n",
    "\n",
    "        model.train(); opt.zero_grad()\n",
    "\n",
    "        lhs = batch.values\n",
    "\n",
    "        l_batch, r_batch, neg_batch = model(lhs)\n",
    "        # nan tests...\n",
    "        l_test = np.isnan(np.mean(l_batch.detach().numpy()))\n",
    "        r_test = np.isnan(np.mean(r_batch.detach().numpy()))\n",
    "        neg_test = np.isnan(np.mean(neg_batch.detach().numpy()))\n",
    "        \n",
    "        if l_test or r_test or neg_test:\n",
    "            nan_break = True\n",
    "            break\n",
    "        \n",
    "        positive_similarity = torch.bmm(l_batch,r_batch.transpose(2,1)) #this is the same as dot product by row\n",
    "\n",
    "        negative_similarity = torch.bmm(l_batch, neg_batch.transpose(2,1)).squeeze(1)\n",
    "\n",
    "        loss = torch.sum(torch.clamp(.1 - positive_similarity + negative_similarity, min=0))\n",
    "        losses.append(loss.detach().numpy())\n",
    "\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "        accuracy_check = positive_similarity.squeeze(1) > negative_similarity[:,0].unsqueeze(1)\n",
    "        acc = np.mean(accuracy_check.detach().numpy())\n",
    "\n",
    "        if i % (print_every*batch_size) == 0:\n",
    "            print('batch avg loss: %s' % str(np.mean(losses[(len(losses)-20):])))\n",
    "        if i % (log_every*batch_size) == 0:\n",
    "            log.info('%s: batch avg loss: %s' % (str(i),str(np.mean(losses[(len(losses)-20):]))))\n",
    "            \n",
    "    if nan_break:\n",
    "        print(\"you've got nans\")\n",
    "        log.warning(\"you've got nans\")\n",
    "        break\n",
    "    \n",
    "    print('Finished epoch %s at %s.' % (epoch,time.ctime()))\n",
    "    log.info(\"Finished epoch %s\" % str(epoch))\n",
    "    \n",
    "    epoch_loss = np.mean(losses[(len(losses)-100):])\n",
    "    \n",
    "    if epoch_loss < min(epoch_losses):\n",
    "        print('best epoch so far!')\n",
    "        log.info('best epoch so far!')\n",
    "        \n",
    "        weights = model.embeddings.weight\n",
    "        with open(OUTPUT_WEIGHTS + 'weights_best_epoch_mush', 'wb') as f:\n",
    "            pickle.dump(weights.data.detach().numpy(), f)\n",
    "    \n",
    "    epoch_losses.append(epoch_loss)\n",
    "    print(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.embeddings.weight\n",
    "with open(OUTPUT_WEIGHTS + 'weights_%s' % SAMPLE_SIZE, 'wb') as f:\n",
    "    pickle.dump(weights.data.detach().numpy(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You made it!')\n",
    "log.info('You made it!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the weights to CSV\n",
    "# weights = model.input_embedding.weight\n",
    "# weights = weights.data.detach().numpy()\n",
    "# np.savetxt(\"weights_%s.csv\" % SAMPLE_SIZE, weights, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# val_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for j in range(0,len(valid), batch_size):\n",
    "#     valperm = torch.randperm(len(valid)).numpy()\n",
    "#     val_indices = valperm[j:j+batch_size]\n",
    "#     val_batch = valid[val_indices]\n",
    "\n",
    "#     val_lhs = val_batch.values\n",
    "\n",
    "#     val_l_batch, val_r_batch, val_neg_batch = model(val_lhs)\n",
    "\n",
    "#     val_positive_similarity = torch.bmm(val_l_batch,val_r_batch.transpose(2,1))\n",
    "#     val_negative_similarity = torch.bmm(val_l_batch, val_neg_batch.transpose(2,1)).squeeze(1)\n",
    "\n",
    "#     val_loss = torch.sum(torch.clamp(.1 - val_positive_similarity + val_negative_similarity, min=0))\n",
    "#     if j % (print_every*batch_size) == 0:\n",
    "#         print(val_loss)\n",
    "    \n",
    "#     val_accuracy_check = val_positive_similarity.squeeze(1) > val_negative_similarity[:,0].unsqueeze(1)\n",
    "#     val_acc += np.sum(val_accuracy_check.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(val_acc/len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# def build_vocab(train, min_ct = 2):\n",
    "#     ''' build vocabulary for an array/list/series of text '''\n",
    "#     # To do: smaller groups before aggregating to improve performance\n",
    "#     def wordcount_df(doc):\n",
    "#         tok = doc.split()\n",
    "#         d = pd.DataFrame.from_dict(Counter(tok),orient='index').reset_index().rename(columns={'index':'word'})\n",
    "#         return d\n",
    "\n",
    "#     d_list = [wordcount_df(x) for x in train]\n",
    "\n",
    "#     d = pd.concat(d_list,axis=0)\n",
    "\n",
    "#     d = d.groupby(['word'])[0].sum().sort_values(ascending=False)\n",
    "#     d = d[d >= min_ct]\n",
    "    \n",
    "#     d = dict(zip(d.index.values, range(len(d))))\n",
    "    \n",
    "#     return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = train[0:100]\n",
    "# validation = train[100:150]\n",
    "# batch_size = 100\n",
    "\n",
    "# model.train()\n",
    "# opt.zero_grad()\n",
    "\n",
    "# lhs = batch.values\n",
    "\n",
    "# l_batch, r_batch, neg_batch = model(lhs)\n",
    "\n",
    "# positive_similarity = torch.bmm(l_batch,r_batch.transpose(2,1)) #this is the same as dot product by row\n",
    "\n",
    "# negative_similarity = torch.bmm(l_batch, neg_batch.transpose(2,1)).squeeze(1)\n",
    "\n",
    "# loss = torch.mean(torch.clamp(.1 - positive_similarity + negative_similarity, min=0))\n",
    "# loss\n",
    "\n",
    "# loss.backward(); opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy of predictions in current batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_check = positive_similarity.squeeze(1) > negative_similarity[:,0].unsqueeze(1)\n",
    "# acc = np.mean(similarity_check.detach().numpy())\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def embed_doc(d,vocab,embedding,normalize=False):\n",
    "#     positions = []\n",
    "#     for t in d:\n",
    "#         try:\n",
    "#             positions.append(vocab[t])\n",
    "#         except KeyError:\n",
    "#             pass\n",
    "#     output = torch.sum(embedding(torch.LongTensor(positions)),dim=0)\n",
    "#     if normalize:\n",
    "#         output = output / output.norm()\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # similarity\n",
    "# l_batch = []\n",
    "# r_batch = []\n",
    "# neg_batch = []\n",
    "\n",
    "# for i in range(len(batch)):\n",
    "#     #Positive similarity\n",
    "#     s = batch.values[i].split('\\t') #sentences\n",
    "#     if type(s) == str: #only one sentence in s\n",
    "#         a = s\n",
    "#         b = s\n",
    "#     else:\n",
    "#         a, b = np.random.choice(s, 2, False)\n",
    "    \n",
    "#     a = a.split()\n",
    "#     b = b.split()\n",
    "    \n",
    "#     a_emb = embed_doc(a,train_vocab,input_embedding,normalize=True)\n",
    "#     b_emb = embed_doc(b,train_vocab,input_embedding,normalize=True)\n",
    "    \n",
    "#     l_batch.append(a_emb)\n",
    "#     r_batch.append(b_emb)\n",
    "\n",
    "#     #Negative similarity\n",
    "#     negs = []\n",
    "#     for _i in range(k * 3):\n",
    "#         index = np.random.choice(len(batch))\n",
    "#         if not index == i: #if it's not from the same document\n",
    "#             c = batch.values[index].split('\\t')\n",
    "#             c = np.random.choice(c, 1)[0].split()\n",
    "#             c_emb = embed_doc(c,train_vocab,input_embedding,normalize=True)\n",
    "#             negs.append(c_emb)\n",
    "#             if(len(negs) >= k):\n",
    "#                 break\n",
    "    \n",
    "#     neg_batch.append(torch.stack(negs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
