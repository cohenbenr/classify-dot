{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from validation.data import dot_train_data, get_soc_n, get_dictionary, indeed_test_data, virginia_test_data\n",
    "from embed_software.preprocess import *\n",
    "from embed_software.utils import get_embeddings, embed_docs\n",
    "from classification.embedding import PreEmbeddedVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pd.set_option('max_colwidth',50)\n",
    "pd.set_option('display.width', 700)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 100000\n",
    "SOC_LEVEL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = dot_train_data(SOC_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from gcsfs import GCSFileSystem\n",
    "\n",
    "fs = GCSFileSystem(project='labor-market-data')\n",
    "with fs.open('lmd-classify-dot/data/us/company-everything.csv') as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "df['title'] = df.title.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test, va_df = virginia_test_data('../data/va_job_posts.json', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def _embed(embedding, d, sentences):\n",
    "    if sentences == True:\n",
    "        fn = embedding.embed_paragraph\n",
    "    else:\n",
    "        fn = embedding.embed_doc\n",
    "\n",
    "    doc = fn(d).T.reshape(1, 100, -1)\n",
    "    return torch.from_numpy(doc).float()\n",
    "\n",
    "def load_source(embedding, X_train, y_train, sentences):\n",
    "    for d,y in zip(X_train, y_train):\n",
    "        doc = _embed(embedding, d, sentences)\n",
    "        label = torch.tensor([y]).long()\n",
    "        yield doc, label\n",
    "\n",
    "def load_target(embedding, docs, sentences):\n",
    "    for d in docs:\n",
    "        yield _embed(embedding, d, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from classification.embedding import Embedding\n",
    "\n",
    "embedding = Embedding('../glove-models/glove-va-100.txt', sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "label_lookup = {v:k for k,v in pd.Series(y_train.unique()).to_dict().items()}\n",
    "y_train_idx = [label_lookup[y] for y in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "docs, labels = zip(*list(load_source(embedding, X_train, y_train_idx, sentences = False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# df = df.sample(n=50000)\n",
    "# target = list(load_target(embedding, df.content, sentences = False))\n",
    "\n",
    "idx = np.random.choice(X_test.index, 100000, replace=False)\n",
    "idx = [i for i in idx if X_test[i] is not None]\n",
    "\n",
    "target = list(load_target(embedding, X_test[idx], sentences = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from toolz import curry\n",
    "from time import perf_counter\n",
    "from math import ceil\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, net, opt, criterion = None):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.opt = opt(net)\n",
    "        self.criterion = criterion\n",
    "        self.net.register_backward_hook(printgradnorm)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X).view(-1)\n",
    "\n",
    "    def evaluate(self, source, target, label):\n",
    "        out = self.__call__(source)\n",
    "        loss = self.criterion(out.reshape(1, -1), label)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Discriminator(Classifier):\n",
    "    def evaluate(self, source, target, label):\n",
    "        guess_s = self.__call__(source)\n",
    "        guess_t = self.__call__(target)\n",
    "        loss = self.criterion(guess_s, torch.tensor([1.]))\n",
    "        loss += self.criterion(guess_t, torch.tensor([0.]))\n",
    "        return loss\n",
    "        \n",
    "\n",
    "class PlatonicNet():\n",
    "    def __init__(self, embedder, classifier, discriminator, batch_size=64, n_epochs=5, grad_norm_clip=0.25):\n",
    "        self.discriminator = discriminator\n",
    "        self.classifier = classifier\n",
    "        self.embedder = embedder\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.grad_norm_clip = grad_norm_clip\n",
    "\n",
    "    def load_data(self, docs, labels, target):\n",
    "        self.docs, self.labels, self.target = docs, labels, target.copy()\n",
    "\n",
    "    def batch(self, size):\n",
    "        random.shuffle(self.target)\n",
    "\n",
    "        dat = list(zip(self.docs, self.labels, self.target))\n",
    "        random.shuffle(dat)\n",
    "\n",
    "        out = []\n",
    "        while dat:\n",
    "            head,dat = dat[:size], dat[size:]\n",
    "            out.append(head)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def epoch(self, embedder):\n",
    "        epoch_disc_loss = 0\n",
    "        epoch_class_loss = 0\n",
    "        epoch_start = perf_counter()\n",
    "\n",
    "        for i,batch in enumerate(self.batch(self.batch_size)):\n",
    "            batch_disc_loss = 0\n",
    "            batch_class_loss = 0\n",
    "\n",
    "            # run for each net, classifier and discriminator\n",
    "            for net,sign in [(self.classifier, 1.)]:\n",
    "\n",
    "                # due to pytorch updating, \n",
    "                # run twice, once for embedder, once for the other model\n",
    "                for updating_model,sgn in [(embedder, sign), (net, 1.)]:\n",
    "                    \n",
    "                    updating_model.opt.zero_grad()\n",
    "                    loss = 0\n",
    "                    for source, label, target in batch:\n",
    "                        loss += net.evaluate(embedder(source), embedder(target), label)\n",
    "                        if torch.isnan(loss):\n",
    "                            print(embedder(source))\n",
    "                            print(loss)\n",
    "                            raise Exception('LOSS/EMBEDDING IS NAN')\n",
    "\n",
    "                    loss *= sign\n",
    "                    \n",
    "                    if sign < 0:\n",
    "                        batch_disc_loss += loss\n",
    "                        epoch_disc_loss += loss\n",
    "                    else:\n",
    "                        batch_class_loss += loss\n",
    "                        epoch_class_loss += loss\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_value_(updating_model.net.parameters(), self.grad_norm_clip)\n",
    "                    updating_model.opt.step()\n",
    "                    \n",
    "            if i % 100 == 0:\n",
    "                print(f'Batch class/disc loss: {batch_class_loss} ::: {batch_disc_loss}')\n",
    "        epoch_time = round((perf_counter() - epoch_start)/60)\n",
    "        print(f'----------- EPOCH --------------\\nEpoch finished in {epoch_time} minutes. class/disc loss: {epoch_class_loss} ::: {epoch_disc_loss}')        \n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self.epoch(self.embedder)            \n",
    "\n",
    "\n",
    "def printgradnorm(self, grad_input, grad_output):\n",
    "    if grad_input[0].norm() > 10.:\n",
    "        print('grad_input norm:', grad_input[0].norm())\n",
    "\n",
    "class GatedNet(torch.nn.Module):\n",
    "    def __init__(self, embed_size, layers):\n",
    "        super().__init__()\n",
    "        self.conver = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_size, out_channels=layers, kernel_size=1, groups=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.conver.register_backward_hook(printgradnorm)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        convs = self.conver(x)\n",
    "        out = torch.matmul(x, torch.t(convs.max(1).values))\n",
    "        return out / torch.norm(out)  \n",
    "\n",
    "class ParallelFilters(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "        for i,net in enumerate(filters):\n",
    "            self.add_module(f'filter_{i}', net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([net(x) for net in self.children()], 1)    \n",
    "\n",
    "\n",
    "def _embedder(embed_size, layers):\n",
    "    filters = [\n",
    "        nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_size, out_channels=out_channels, kernel_size=kernel_size, groups=1, padding=kernel_size - 1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1))\n",
    "        for kernel_size,out_channels in layers]\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        ParallelFilters(filters),\n",
    "        nn.Dropout(p=0.25)\n",
    "    )\n",
    "\n",
    "    net.register_backward_hook(printgradnorm)\n",
    "    return net    \n",
    "\n",
    "def _embedder_single(embed_size, out_channels):\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=embed_size, out_channels=out_channels, kernel_size=1, groups=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.AdaptiveMaxPool1d(output_size=1),\n",
    "        nn.Dropout(p=0.25)\n",
    "    )\n",
    "\n",
    "    net.register_backward_hook(printgradnorm)\n",
    "    return net    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "from adabound import AdaBound\n",
    "\n",
    "@curry\n",
    "def adam_opt(lr, net):\n",
    "    return optim.Adam(net.parameters(), lr=lr, weight_decay=1.0)\n",
    "\n",
    "@curry\n",
    "def ab_opt(lr, net):\n",
    "    return AdaBound(net.parameters(), lr=lr, final_lr=0.01, weight_decay=1.0)\n",
    "\n",
    "n_classes = y_train.unique().shape[0]\n",
    "\n",
    "filters = [(1, 50), (2, 50), (3, 50), (4, 50)]\n",
    "final_layer_size = np.sum([f[1] for f in filters])\n",
    "\n",
    "print(final_layer_size)\n",
    "\n",
    "embedder = Classifier(_embedder(100, filters), ab_opt(0.0001))\n",
    "classifier = Classifier(nn.Sequential(nn.Linear(final_layer_size, n_classes)), ab_opt(0.0001), nn.CrossEntropyLoss())\n",
    "discriminator = Discriminator(nn.Sequential(nn.Linear(final_layer_size, 1)), ab_opt(0.0001), nn.BCELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = PlatonicNet(embedder, classifier, discriminator, n_epochs=30, grad_norm_clip=0.1)\n",
    "model.load_data(docs, labels, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "i = 90\n",
    "\n",
    "def get_spread(d):\n",
    "    vals = model.embedder.net.conver(d).max(1).values.detach().numpy()\n",
    "    return vals.max() - vals.min()\n",
    "    \n",
    "\n",
    "np.mean([get_spread(d) for d in docs[:500]]), np.mean([get_spread(d) for d in target[:500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "i = 44\n",
    "\n",
    "idx = np.where(model.embedder.net.conver(target[i]).max(1).values.detach().numpy() < .4)[1]\n",
    "np.array(df.content.iloc[i].split('\\t'))[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from validation.scoring import bubbleup_score\n",
    "\n",
    "def simple_embed(doc):\n",
    "    X = doc.sum(2).reshape(-1)\n",
    "    return X / torch.norm(X)\n",
    "\n",
    "def ss_embed(doc):\n",
    "    d = embedding.embed_doc(doc).sum(0)\n",
    "    return d / np.linalg.norm(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Xe_train = [simple_embed(d).detach().numpy() for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43469912354052465"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1., n_jobs=-1, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "clf.fit(Xe_train, y_train)\n",
    "preds = clf.predict(Xe_train)\n",
    "accuracy_score(preds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ss_embed('manager of sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1731868108753034"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(ss_embed('manager of farm labour'), ss_embed('sales clerk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10387624967646984"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(ss_embed('manager of sales personel'), ss_embed('manager of farm labour'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2131845341814802"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(ss_embed('manager of sales personel'), ss_embed('sales clerk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3484722905719662"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(ss_embed('sales personel'), ss_embed('sales clerk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "labels = sorted(y_train.unique())\n",
    "\n",
    "sorted(labels)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6,   4,  44, 274,  49,  28,  22,   0,   5,  72,   6,  19,  16,\n",
       "         9,   6,  19,   0,   1,   1,  52,  24,   1,   1,   1,   0,  13,\n",
       "         4,   1,  38,   8,   0,   0,   0,   0,   4,   0,  13,  13,   5,\n",
       "         2,   6,   0,   0,   2,   4,   3,   1,  14,  15,   1,   0,   0,\n",
       "         6,   0,   3,   5,   0,   5,   7,   0,   1,  13,   6,   0,   1,\n",
       "         4,  25,   0,   0,   6,  10,   0,   4,   3,   1,   1,   1,   3,\n",
       "         5,   0,   1,   1,   0,   0,   0,   2,  17,   1,   0,   0,   0,\n",
       "         2,   0,   1,   0,   0])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "confusion_matrix(y_train, preds)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Xp_train = [model.embedder(d).detach().numpy() for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30146489516553626"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1., n_jobs=-1, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "clf.fit(Xp_train, y_train)\n",
    "preds = clf.predict(Xp_train)\n",
    "accuracy_score(preds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE=100000\n",
    "X_test, y_test, ids = indeed_test_data('../data/us/everything.csv', SAMPLE_SIZE, 6)\n",
    "X_train, y_train = dot_train_data(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Xe_test = [model.embedder(d).detach().numpy() for d in load_target(X_test)]\n",
    "Xe_train = [model.embedder(d).detach().numpy() for d in load_target(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=-1, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=5., n_jobs=-1, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "clf.fit(Xe_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46860896376066846"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bubbleup_score(y_train, Xe_test, y_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Xe_test = [simple_embed(d).detach().numpy() for d in load_target(X_test)]\n",
    "Xe_train = [simple_embed(d).detach().numpy() for d in load_target(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=-1, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=5., n_jobs=-1, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(Xe_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4752223066267483"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bubbleup_score(y_train, Xe_test, y_test, clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "name": "platonic-embedding.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
