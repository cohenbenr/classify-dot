{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ! pip install --quiet seaborn toolz fuzzywuzzy\n",
    "#  ! pip install -e 'git://github.com/nandanrao/embed-software.git#egg=embed_software'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gcsfs import GCSFileSystem\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "#import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import InnerProductSimilarity, MarginRankingLoss, StarSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='bcohen1.log',format='%(asctime)s %(message)s', datefmt='%Y-%m-%d %H:%M:%S %p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 100000\n",
    "SOC_LEVEL = 2\n",
    "BUBBLE_UP = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indeed_texts(path, use_gcs = False, **kwargs):\n",
    "    \"\"\"Reads csv with indeed data that turns into test set\"\"\"\n",
    "    if use_gcs:\n",
    "        fs = GCSFileSystem(project='labor-market-data')\n",
    "        path = path.replace('..','lmd-classify-dot',1)\n",
    "        with fs.open(path) as f:\n",
    "            indeed = pd.read_csv(f, **kwargs)\n",
    "    else:\n",
    "        indeed = pd.read_csv(path, **kwargs)\n",
    "\n",
    "    indeed['title'] = indeed.title.str.lower()\n",
    "    return indeed\n",
    "\n",
    "def indeed_test_data(texts, lim, soc_n, use_gcs = False):\n",
    "    \"\"\"Make test data from indeed (pre-embedded)\"\"\"\n",
    "    indeed = get_indeed_texts(texts, use_gcs, nrows=lim)\n",
    "    matcher = make_matcher()\n",
    "    matches = matcher(indeed.reset_index()).set_index('index')\n",
    "    return matches.content, get_soc_n(matches.code, soc_n), matches.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed = get_indeed_texts('../data/us/everything.csv',use_gcs=True,nrows=SAMPLE_SIZE)\n",
    "indeed = indeed['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.sample(range(len(indeed)),int(.8*len(indeed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    description of duties the client services spec...\n",
       "1    part time must be graduate of an accredited pr...\n",
       "2    part time personal care assistants grow with u...\n",
       "3    scientist building product formulations lookin...\n",
       "4    qualified applicants must be at least 21 years...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = indeed[idx]\n",
    "train.reset_index(drop=True,inplace=True)\n",
    "\n",
    "valid = indeed.drop(idx)\n",
    "valid.reset_index(drop=True,inplace=True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(train, min_ct = 2):\n",
    "    ''' build vocabulary for an array/list/series of text '''\n",
    "    # To do: smaller groups before aggregating to improve performance\n",
    "    def wordcount_df(doc):\n",
    "        tok = doc.split()\n",
    "        d = pd.DataFrame.from_dict(Counter(tok),orient='index').reset_index().rename(columns={'index':'word'})\n",
    "        return d\n",
    "\n",
    "    d_list = [wordcount_df(x) for x in train]\n",
    "\n",
    "    d = pd.concat(d_list,axis=0)\n",
    "\n",
    "    d = d.groupby(['word'])[0].sum().sort_values(ascending=False)\n",
    "    d = d[d >= min_ct]\n",
    "    \n",
    "    d = dict(zip(d.index.values, range(len(d))))\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .01\n",
    "\n",
    "train_vocab = build_vocab(indeed)\n",
    "\n",
    "pd.DataFrame.from_dict(train_vocab,orient='index').to_csv('train_vocab_%s.csv' % SAMPLE_SIZE)\n",
    "\n",
    "model = StarSpace(\n",
    "    d_embed=100,\n",
    "    vocabulary=train_vocab,\n",
    "    n_input=len(train_vocab),\n",
    "    k_neg = 10,\n",
    "    max_norm=20)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "batch_size = 100\n",
    "#for epoch in range(epochs):\n",
    "#shuffle order of training data and validation data\n",
    "\n",
    "epochs = 5\n",
    "iterations = 0\n",
    "start = time.time()\n",
    "best_val_acc = -1\n",
    "print_every = 100\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1770820.2500, grad_fn=<SumBackward0>)\n",
      "tensor(832375.3750, grad_fn=<SumBackward0>)\n",
      "tensor(52969.1641, grad_fn=<SumBackward0>)\n",
      "tensor(117691.8828, grad_fn=<SumBackward0>)\n",
      "tensor(194385.6406, grad_fn=<SumBackward0>)\n",
      "tensor(166289.3438, grad_fn=<SumBackward0>)\n",
      "tensor(59307.4297, grad_fn=<SumBackward0>)\n",
      "tensor(3915.5503, grad_fn=<SumBackward0>)\n",
      "Finished epoch 0 at Mon Jun  8 09:41:11 2020.\n",
      "tensor(31230.6992, grad_fn=<SumBackward0>)\n",
      "tensor(689107.1875, grad_fn=<SumBackward0>)\n",
      "tensor(41807.2656, grad_fn=<SumBackward0>)\n",
      "tensor(111490.8984, grad_fn=<SumBackward0>)\n",
      "tensor(15529.7617, grad_fn=<SumBackward0>)\n",
      "tensor(86387.2109, grad_fn=<SumBackward0>)\n",
      "tensor(290320.3438, grad_fn=<SumBackward0>)\n",
      "tensor(42863.6562, grad_fn=<SumBackward0>)\n",
      "Finished epoch 1 at Mon Jun  8 10:44:40 2020.\n",
      "tensor(46118.9414, grad_fn=<SumBackward0>)\n",
      "tensor(3284.4058, grad_fn=<SumBackward0>)\n",
      "tensor(3693.9802, grad_fn=<SumBackward0>)\n",
      "tensor(26267.3203, grad_fn=<SumBackward0>)\n",
      "tensor(31527.9551, grad_fn=<SumBackward0>)\n",
      "tensor(8573.5430, grad_fn=<SumBackward0>)\n",
      "tensor(8123.8179, grad_fn=<SumBackward0>)\n",
      "tensor(20292.8438, grad_fn=<SumBackward0>)\n",
      "Finished epoch 2 at Mon Jun  8 11:52:18 2020.\n",
      "tensor(34383.8008, grad_fn=<SumBackward0>)\n",
      "tensor(38624.8555, grad_fn=<SumBackward0>)\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "tensor(54544.6641, grad_fn=<SumBackward0>)\n",
      "tensor(6434.3506, grad_fn=<SumBackward0>)\n",
      "tensor(166208.7031, grad_fn=<SumBackward0>)\n",
      "tensor(7570.8140, grad_fn=<SumBackward0>)\n",
      "tensor(22.7736, grad_fn=<SumBackward0>)\n",
      "Finished epoch 3 at Mon Jun  8 12:59:07 2020.\n",
      "tensor(53126.0469, grad_fn=<SumBackward0>)\n",
      "tensor(58861.7891, grad_fn=<SumBackward0>)\n",
      "tensor(121465.9766, grad_fn=<SumBackward0>)\n",
      "tensor(81438.0938, grad_fn=<SumBackward0>)\n",
      "tensor(25926.9922, grad_fn=<SumBackward0>)\n",
      "tensor(9212.8926, grad_fn=<SumBackward0>)\n",
      "tensor(21444.2148, grad_fn=<SumBackward0>)\n",
      "tensor(123347.2266, grad_fn=<SumBackward0>)\n",
      "Finished epoch 4 at Mon Jun  8 14:06:40 2020.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(len(train)).numpy()\n",
    "\n",
    "    for i in range(0,len(train), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch = train[indices]\n",
    "\n",
    "        model.train(); opt.zero_grad()\n",
    "\n",
    "        lhs = batch.values\n",
    "\n",
    "        l_batch, r_batch, neg_batch = model(lhs)\n",
    "\n",
    "        positive_similarity = torch.bmm(l_batch,r_batch.transpose(2,1)) #this is the same as dot product by row\n",
    "\n",
    "        negative_similarity = torch.bmm(l_batch, neg_batch.transpose(2,1)).squeeze(1)\n",
    "\n",
    "        loss = torch.sum(torch.clamp(.1 - positive_similarity + negative_similarity, min=0))\n",
    "        losses.append(loss.detach().numpy())\n",
    "\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "        accuracy_check = positive_similarity.squeeze(1) > negative_similarity[:,0].unsqueeze(1)\n",
    "        acc = np.mean(accuracy_check.detach().numpy())\n",
    "\n",
    "        if i % (print_every*batch_size) == 0:\n",
    "            print(loss)\n",
    "    \n",
    "    print('Finished epoch %s at %s.' % (epoch,time.ctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the weights to CSV\n",
    "weights = model.input_embedding.weight\n",
    "weights = weights.data.detach().numpy()\n",
    "np.savetxt(\"weights_%s.csv\" % SAMPLE_SIZE, weights, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5317.2021, grad_fn=<SumBackward0>)\n",
      "tensor(1226.6842, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for j in range(0,len(valid), batch_size):\n",
    "    valperm = torch.randperm(len(valid)).numpy()\n",
    "    val_indices = valperm[j:j+batch_size]\n",
    "    val_batch = valid[val_indices]\n",
    "\n",
    "    val_lhs = val_batch.values\n",
    "\n",
    "    val_l_batch, val_r_batch, val_neg_batch = model(val_lhs)\n",
    "\n",
    "    val_positive_similarity = torch.bmm(val_l_batch,val_r_batch.transpose(2,1))\n",
    "    val_negative_similarity = torch.bmm(val_l_batch, val_neg_batch.transpose(2,1)).squeeze(1)\n",
    "\n",
    "    val_loss = torch.sum(torch.clamp(.1 - val_positive_similarity + val_negative_similarity, min=0))\n",
    "    if j % (print_every*batch_size) == 0:\n",
    "        print(val_loss)\n",
    "    \n",
    "    val_accuracy_check = val_positive_similarity.squeeze(1) > val_negative_similarity[:,0].unsqueeze(1)\n",
    "    val_acc += np.sum(val_accuracy_check.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99935\n"
     ]
    }
   ],
   "source": [
    "print(val_acc/len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = train[0:100]\n",
    "# validation = train[100:150]\n",
    "# batch_size = 100\n",
    "\n",
    "# model.train()\n",
    "# opt.zero_grad()\n",
    "\n",
    "# lhs = batch.values\n",
    "\n",
    "# l_batch, r_batch, neg_batch = model(lhs)\n",
    "\n",
    "# positive_similarity = torch.bmm(l_batch,r_batch.transpose(2,1)) #this is the same as dot product by row\n",
    "\n",
    "# negative_similarity = torch.bmm(l_batch, neg_batch.transpose(2,1)).squeeze(1)\n",
    "\n",
    "# loss = torch.mean(torch.clamp(.1 - positive_similarity + negative_similarity, min=0))\n",
    "# loss\n",
    "\n",
    "# loss.backward(); opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy of predictions in current batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_check = positive_similarity.squeeze(1) > negative_similarity[:,0].unsqueeze(1)\n",
    "acc = np.mean(similarity_check.detach().numpy())\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def embed_doc(d,vocab,embedding,normalize=False):\n",
    "#     positions = []\n",
    "#     for t in d:\n",
    "#         try:\n",
    "#             positions.append(vocab[t])\n",
    "#         except KeyError:\n",
    "#             pass\n",
    "#     output = torch.sum(embedding(torch.LongTensor(positions)),dim=0)\n",
    "#     if normalize:\n",
    "#         output = output / output.norm()\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # similarity\n",
    "# l_batch = []\n",
    "# r_batch = []\n",
    "# neg_batch = []\n",
    "\n",
    "# for i in range(len(batch)):\n",
    "#     #Positive similarity\n",
    "#     s = batch.values[i].split('\\t') #sentences\n",
    "#     if type(s) == str: #only one sentence in s\n",
    "#         a = s\n",
    "#         b = s\n",
    "#     else:\n",
    "#         a, b = np.random.choice(s, 2, False)\n",
    "    \n",
    "#     a = a.split()\n",
    "#     b = b.split()\n",
    "    \n",
    "#     a_emb = embed_doc(a,train_vocab,input_embedding,normalize=True)\n",
    "#     b_emb = embed_doc(b,train_vocab,input_embedding,normalize=True)\n",
    "    \n",
    "#     l_batch.append(a_emb)\n",
    "#     r_batch.append(b_emb)\n",
    "\n",
    "#     #Negative similarity\n",
    "#     negs = []\n",
    "#     for _i in range(k * 3):\n",
    "#         index = np.random.choice(len(batch))\n",
    "#         if not index == i: #if it's not from the same document\n",
    "#             c = batch.values[index].split('\\t')\n",
    "#             c = np.random.choice(c, 1)[0].split()\n",
    "#             c_emb = embed_doc(c,train_vocab,input_embedding,normalize=True)\n",
    "#             negs.append(c_emb)\n",
    "#             if(len(negs) >= k):\n",
    "#                 break\n",
    "    \n",
    "#     neg_batch.append(torch.stack(negs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
