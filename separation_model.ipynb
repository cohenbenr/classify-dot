{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -e 'git://github.com/nandanrao/embed-software.git#egg=embed_software'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --quiet fuzzywuzzy gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gcsfs import GCSFileSystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "#import torchtext\n",
    "\n",
    "from validation.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import StarSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 10000\n",
    "SOC_LEVEL = 3\n",
    "OUTPUT_WEIGHTS = 'data/separation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f511b5ea190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(fmt=\"%(asctime)s %(levelname)s: %(message)s\", \n",
    "                          datefmt=\"%Y-%m-%d - %H:%M:%S\")\n",
    "fh = logging.FileHandler(\"separation_model.log\", \"w\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_test(l_batch, r_batch, neg_batch):\n",
    "    global nan_break\n",
    "    \n",
    "    l_test = np.isnan(np.mean(l_batch.detach().cpu().numpy()))\n",
    "    r_test = np.isnan(np.mean(r_batch.detach().cpu().numpy()))\n",
    "    neg_test = np.isnan(np.mean(neg_batch.detach().cpu().numpy()))\n",
    "    if l_test or r_test or neg_test:\n",
    "        nan_break = True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('Pulling Indeed data for sample size %s' % SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job ads data\n",
    "indeed = get_indeed_texts('../data/us/everything.csv',use_gcs=True,nrows=SAMPLE_SIZE)\n",
    "indeed = indeed['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = indeed.copy()\n",
    "del indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DOT classifications data\n",
    "dot, dot_labs = dot_train_data(SOC_LEVEL)\n",
    "\n",
    "dot = dot.reset_index(drop=True)\n",
    "dot_labs = dot_labs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('About to train vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer(min_df = 10,\n",
    "                             max_df = .99)\n",
    "Vectorizer.fit(train)\n",
    "\n",
    "train_vocab = Vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8881"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('Trained Vocab of size %s' % str(len(train_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the file\n",
    "with open(OUTPUT_WEIGHTS + 'train_vocab_%s' % SAMPLE_SIZE, 'wb') as f:\n",
    "    pickle.dump(train_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start from file:\n",
    "# with open('data/separation/weights_100000', 'rb') as f:\n",
    "#     embeddings = pickle.load(f)\n",
    "\n",
    "# print(embeddings.shape)\n",
    "# embeddings = torch.FloatTensor(embeddings)\n",
    "# embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "\n",
    "# with open('data/separation/train_vocab_100000', 'rb') as f:\n",
    "#     vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StarSpace(\n",
       "  (embeddings): Embedding(8881, 100, max_norm=20)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StarSpace(\n",
    "    d_embed=100,\n",
    "    vocabulary=train_vocab,\n",
    "    k_neg = 10)\n",
    "#     input_embedding = embeddings)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR params\n",
    "clusters_to_sample = 96\n",
    "lr_steps = 1\n",
    "cluster_nums = np.array(list(set(dot_labs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(100,clusters_to_sample)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#lr_optim = torch.optim.SGD(LR.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .01\n",
    "opt = torch.optim.Adam(list(model.parameters()) + list(LR.parameters()), lr=lr)\n",
    "#opt = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = model.get_positions(train)\n",
    "dot_positions = model.get_positions(dot)\n",
    "\n",
    "for i in range(len(train_pos)):\n",
    "    for j in range(len(train_pos[i])):\n",
    "        train_pos[i][j] = train_pos[i][j].to(device)\n",
    "\n",
    "for i in range(len(dot_positions)):\n",
    "    for j in range(len(dot_positions[i])):\n",
    "        dot_positions[i][j] = dot_positions[i][j].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run parameters\n",
    "epochs = 1\n",
    "print_every = 10\n",
    "log_every = 10\n",
    "batch_size = 100\n",
    "\n",
    "#Losses\n",
    "losses = []\n",
    "separation_losses = []\n",
    "epoch_losses = [1e12]\n",
    "log.info('Beginning run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_sample = dot_positions.copy()\n",
    "dot_y_sample = dot_labs.copy()\n",
    "dot_y_enc = torch.tensor(np.unique(dot_y_sample,return_inverse=True)[1]) #encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "separation avg loss: 15.084224\n",
      "star avg loss: 54150616.0\n",
      "separation avg loss: 12.624626\n",
      "star avg loss: 26076774.0\n",
      "separation avg loss: 12.072457\n",
      "star avg loss: 16078338.0\n",
      "separation avg loss: 11.435955\n",
      "star avg loss: 13919906.0\n",
      "separation avg loss: 11.721858\n",
      "star avg loss: 11736738.0\n",
      "separation avg loss: 11.148764\n",
      "star avg loss: 12945584.0\n",
      "separation avg loss: 11.313104\n",
      "star avg loss: 11272147.0\n",
      "separation avg loss: 11.1434\n",
      "star avg loss: 12001306.0\n",
      "separation avg loss: 10.984521\n",
      "star avg loss: 9854323.0\n",
      "separation avg loss: 10.978853\n",
      "star avg loss: 10751090.0\n",
      "Finished epoch 0 at Mon Jun 22 13:57:25 2020.\n",
      "best epoch so far!\n",
      "[1000000000000.0, 13814627.0]\n"
     ]
    }
   ],
   "source": [
    "#Real loop\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(len(train_pos)).numpy()\n",
    "    nan_break = False\n",
    "    \n",
    "    for i in range(0,len(train), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch = train_pos[indices]\n",
    "\n",
    "        model.train(); opt.zero_grad()\n",
    "        \n",
    "        l_batch, r_batch, neg_batch = model(batch)\n",
    "        \n",
    "        #Test for nans\n",
    "        if nan_test(l_batch, r_batch, neg_batch):\n",
    "            break\n",
    "        \n",
    "        positive_similarity = torch.bmm(l_batch,r_batch.transpose(2,1))\n",
    "        negative_similarity = torch.bmm(l_batch, neg_batch.transpose(2,1)).squeeze(1)\n",
    "\n",
    "        star_loss = torch.sum(torch.clamp(.1 - positive_similarity + negative_similarity, min=0))\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in LR.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Now add in clustering loss for DOT categories\n",
    "        #clusts = np.random.choice(cluster_nums, clusters_to_sample, False)\n",
    "        #mask = np.in1d(dot_labs,clusts)\n",
    "        \n",
    "#         dot_sample = dot_positions[mask].copy()\n",
    "#         dot_y_sample = dot_labs[mask].reset_index(drop=True)\n",
    "\n",
    "#         dot_y_enc = torch.tensor(np.unique(dot_y_sample,return_inverse=True)[1]) #encoded\n",
    "        \n",
    "        LR = LogisticRegression(100,clusters_to_sample)\n",
    "        new_dots = [torch.cat(doc) for doc in dot_sample]\n",
    "        dot_emb = [model.embed_doc(doc) for doc in new_dots]\n",
    "        dot_emb = torch.stack(dot_emb)\n",
    "\n",
    "        for r in range(lr_steps):\n",
    "            opt.zero_grad()\n",
    "\n",
    "            output = LR(dot_emb)\n",
    "            separation_loss = criterion(output,dot_y_enc.clone())\n",
    "            separation_loss.backward(retain_graph=True)\n",
    "            opt.step()         \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for param in LR.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        #lr_optim.zero_grad()\n",
    "        \n",
    "        #Combine losses\n",
    "        loss = star_loss + separation_loss\n",
    "\n",
    "        loss.backward();opt.step()\n",
    "\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        separation_losses.append(separation_loss.detach().cpu().numpy())\n",
    "\n",
    "        if i % (print_every*batch_size) == 0:\n",
    "            print('separation avg loss: %s' % str(np.mean(separation_losses[-10:])))\n",
    "            print('star avg loss: %s' % str(np.mean(losses[-10:])))\n",
    "        if i % (log_every*batch_size) == 0:\n",
    "            log.info('separation avg loss: %s' % str(np.mean(separation_losses[-10:])))\n",
    "            log.info('star avg loss: %s' % str(np.mean(losses[-10:])))\n",
    "    \n",
    "    # End of inner loop\n",
    "    if nan_break:\n",
    "        print(\"you've got nans\")\n",
    "        log.warning(\"you've got nans\")\n",
    "        break\n",
    "    \n",
    "    print('Finished epoch %s at %s.' % (epoch,time.ctime()))\n",
    "    log.info(\"Finished epoch %s\" % str(epoch))\n",
    "    \n",
    "    epoch_loss = np.mean(losses[(len(losses)-100):])\n",
    "    \n",
    "    if epoch_loss < min(epoch_losses):\n",
    "        print('best epoch so far!')\n",
    "        log.info('best epoch so far!')\n",
    "        \n",
    "        weights = model.embeddings.weight\n",
    "        with open(OUTPUT_WEIGHTS + 'weights_best_epoch', 'wb') as f:\n",
    "            pickle.dump(weights.data.detach().cpu().numpy(), f)\n",
    "    \n",
    "    epoch_losses.append(epoch_loss)\n",
    "    print(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.embeddings.weight\n",
    "with open(OUTPUT_WEIGHTS + 'weights_%s' % SAMPLE_SIZE, 'wb') as f:\n",
    "    pickle.dump(weights.data.detach().cpu().numpy(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You made it!\n"
     ]
    }
   ],
   "source": [
    "print('You made it!')\n",
    "log.info('You made it!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the weights to CSV\n",
    "# weights = model.input_embedding.weight\n",
    "# weights = weights.data.detach().numpy()\n",
    "# np.savetxt(\"weights_%s.csv\" % SAMPLE_SIZE, weights, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
